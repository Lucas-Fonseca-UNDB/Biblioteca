# Guia Completo de Retrieval-Augmented Generation (RAG)
## Um Curso Estruturado sobre IA, LLMs e Sistemas de RecuperaÃ§Ã£o de InformaÃ§Ã£o

---

## ğŸ“‹ Ãndice Geral

1. [MÃ³dulo 1: Fundamentos de RAG](#mÃ³dulo-1-fundamentos-de-rag)
2. [MÃ³dulo 2: Arquitetura e Componentes Principais](#mÃ³dulo-2-arquitetura-e-componentes-principais)
3. [MÃ³dulo 3: Embeddings e RecuperaÃ§Ã£o](#mÃ³dulo-3-embeddings-e-recuperaÃ§Ã£o)
4. [MÃ³dulo 4: IntegraÃ§Ã£o com Modelos de Linguagem](#mÃ³dulo-4-integraÃ§Ã£o-com-modelos-de-linguagem)
5. [MÃ³dulo 5: ImplementaÃ§Ã£o PrÃ¡tica](#mÃ³dulo-5-implementaÃ§Ã£o-prÃ¡tica)
6. [MÃ³dulo 6: AvaliaÃ§Ã£o e MÃ©tricas](#mÃ³dulo-6-avaliaÃ§Ã£o-e-mÃ©tricas)
7. [MÃ³dulo 7: Casos de Uso e Melhores PrÃ¡ticas](#mÃ³dulo-7-casos-de-uso-e-melhores-prÃ¡ticas)
8. [MÃ³dulo 8: TÃ©cnicas AvanÃ§adas](#mÃ³dulo-8-tÃ©cnicas-avanÃ§adas)

---

## **MÃ“DULO 1: FUNDAMENTOS DE RAG**

### Objetivos de Aprendizado
- Compreender a motivaÃ§Ã£o por trÃ¡s do RAG
- Diferenciar RAG de abordagens tradicionais de QA e LLMs puros
- Identificar os limites dos LLMs e como RAG os resolve
- Reconhecer benefÃ­cios e limitaÃ§Ãµes do paradigma RAG

### 1.1 O que Ã© RAG e Por Que Surgiu

**Retrieval-Augmented Generation (RAG)** Ã© um paradigma que combina dois componentes fundamentais:

1. **Retriever**: Um mecanismo que busca informaÃ§Ãµes relevantes de uma fonte externa de conhecimento
2. **Generator**: Um modelo de linguagem que produz respostas usando tanto a query do usuÃ¡rio quanto o contexto recuperado

A necessidade de RAG surgiu de limitaÃ§Ãµes crÃ­ticas dos LLMs modernos:

#### LimitaÃ§Ãµes dos LLMs Puros

| LimitaÃ§Ã£o | Problema | Exemplo |
|-----------|----------|---------|
| **Conhecimento estÃ¡tico**| Treinados apenas com dados histÃ³ricos; nÃ£o conhecem eventos recentes | Perguntar sobre notÃ­cias de hoje a um GPT-3 treinado atÃ© 2021 |
| **AlucinaÃ§Ã£o (Hallucination)**| O modelo inventa informaÃ§Ãµes quando nÃ£o tem conhecimento | LLM responde com confianÃ§a um fato falso sobre uma empresa especÃ­fica |
| **Falta de contexto especÃ­fico**| Desconhecimento de dados proprietÃ¡rios da organizaÃ§Ã£o | Um chatbot corporativo sem acesso aos manuais internos |
| **Impossibilidade de atualizaÃ§Ã£o rÃ¡pida**| Retreinar Ã© custoso e lento | Incorporar novo conhecimento Ã  medida que Ã© publicado |
| **Problema de "distribuiÃ§Ã£o de conhecimento"**| O conhecimento estÃ¡ espalhado nos parÃ¢metros; difÃ­cil de rastrear fontes | "De onde vocÃª tirou isso?" â†’ ImpossÃ­vel citar a origem |

**RAG resolve esses problemas transformando o LLM em um "sistema de leitura ativa"**: em vez de depender apenas do conhecimento memorizado, o modelo pode *buscar* informaÃ§Ã£o relevante em tempo real e entÃ£o *gerar* respostas baseadas nela.

### 1.2 DiferenÃ§a Entre RAG e Abordagens Tradicionais

#### GeraÃ§Ã£o Pura vs. GeraÃ§Ã£o com RecuperaÃ§Ã£o

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LLM Tradicional (Puro)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  User Query â†’ [LLM com conhecimento parametrizado] â†’ Resposta   â”‚
â”‚                     â†“                                           â”‚
â”‚            (Risco alto de alucinaÃ§Ã£o)                           â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Retrieval-Augmented Generation (RAG)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  User Query â†’ [Retriever] â†’ Contexto Relevante                       â”‚
â”‚       â†“                              â†“                               â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ [LLM] â† Contexto Recuperado â†’ Resposta Grounded   â”‚
â”‚                     â†“                                                â”‚
â”‚            (Resposta factualmente precisa com citaÃ§Ãµes)              â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### ComparaÃ§Ã£o com MÃ©todos ClÃ¡ssicos de QA

|      Aspecto        | QA ClÃ¡ssico (Extractive)  |      LLM Puro        |              RAG             |
|---------------------|---------------------------|----------------------|------------------------------|
| **Entrada**         | Query + Documentos        | Query apenas         | Query + Base de Conhecimento |
| **SaÃ­da**           | Span de texto (extrativo) | Texto livre gerado   | Texto livre com grounding    |
| **Fonte de Verdade**| Documentos fornecidos     | ParÃ¢metros do modelo | Documentos + ParÃ¢metros      |
| **Flexibilidade**   | Baixa (limitado a spans)  | Alta (geraÃ§Ã£o livre) | Alta + Factual               |
| **Factualidade**    | Alta (se doc existe)      | Baixa (alucinaÃ§Ã£o)   | Alta (documento rastreÃ¡vel)  |
| **Rastreabilidade** | Sim (span do doc)         | NÃ£o                  | Sim (documentos citÃ¡veis)    |

### 1.3 BenefÃ­cios e LimitaÃ§Ãµes do RAG

#### âœ… BenefÃ­cios

1. **ReduÃ§Ã£o de AlucinaÃ§Ãµes**: Responses grounded em dados reais
2. **Conhecimento Atualizado**: IntegraÃ§Ã£o com dados em tempo real
3. **Rastreabilidade**: Citations mostram a origem da informaÃ§Ã£o
4. **Custo-Efetivo**: NÃ£o requer fine-tuning completo; usa modelos prÃ©-treinados
5. **Flexibilidade**: FÃ¡cil atualizar a base de conhecimento sem retreinar
6. **Privacidade**: Dados sensÃ­veis podem ficar em repos locais (sem enviÃ¡-los ao LLM)
7. **Autoridade de DomÃ­nio**: Incorpora conhecimento especÃ­fico do domÃ­nio

#### âŒ LimitaÃ§Ãµes

1. **LatÃªncia**: Requer operaÃ§Ã£o de busca (trade-off velocidade vs. qualidade)
2. **Qualidade de RecuperaÃ§Ã£o**: Se o retriever falha, o gerador nÃ£o consegue compensar ("garbage in, garbage out")
3. **Contexto Limitado**: LLMs tÃªm janela de contexto finita; nem sempre conseguem usar todo o contexto recuperado
4. **Ranking de RelevÃ¢ncia**: Documentos irrelevantes no top-K podem confundir o LLM
5. **Custo Computacional**: ManutenÃ§Ã£o de bases vetoriais e pipelines de busca
6. **Complexidade de AvaliaÃ§Ã£o**: DifÃ­cil distinguir se o erro Ã© do retriever ou do gerador

### 1.4 Casos de Uso Motivadores

|         Caso de Uso          |               Contexto                 |                 BenefÃ­cio do RAG                       |
|------------------------------|----------------------------------------|--------------------------------------------------------|
| **Q&A Aberto (Open-Domain)** | Perguntas sobre fatos gerais           | Acesso a Wikipedia/web em tempo real                   |
| **Busca Corporativa**        | FuncionÃ¡rios buscam polÃ­ticas internas | Respostas precisas sobre documentos proprietÃ¡rios      |
| **Suporte TÃ©cnico**          | Chatbots respondendo tickets           | ReferÃªncia a manuais e FAQs; reduz erros               |
| **AnÃ¡lise de Documentos**    | Revisar contatos legais, pesquisas     | ExtraÃ§Ã£o de informaÃ§Ã£o contextualizada                 |
| **E-commerce**               | RecomendaÃ§Ãµes de produtos              | Busca semÃ¢ntica + geraÃ§Ã£o de descriÃ§Ãµes personalizadas |
| **Healthcare**               | AssistÃªncia diagnÃ³stica                | Busca de literatura mÃ©dica + raciocÃ­nio do LLM         |

---

## **MÃ“DULO 2: ARQUITETURA E COMPONENTES PRINCIPAIS**

### Objetivos de Aprendizado
- Entender a estrutura end-to-end de um sistema RAG
- Identificar cada componente e sua funÃ§Ã£o
- Reconhecer variaÃ§Ãµes arquiteturais (RAG-Sequence vs. RAG-Token)
- Compreender o fluxo de dados passo a passo

### 2.1 Estrutura Geral: Componentes Principais

Um sistema RAG clÃ¡ssico possui 4 componentes interdependentes:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      ARQUITETURA RAG                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                â”‚
â”‚  â”‚  Documentos  â”‚                                                â”‚
â”‚  â”‚  (PDFs, web, â”‚                                                â”‚
â”‚  â”‚   DB, APIs)  â”‚                                                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                                â”‚
â”‚         â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚         â”œâ”€â†’ 1. INDEXADOR (Preprocessamento) â”‚                    â”‚
â”‚         â”‚ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚         â”‚       â”‚                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚  â”‚  2. BANCO VETORIAL             â”‚                              â”‚
â”‚  â”‚  (Vector Store: FAISS, Chroma, â”‚                              â”‚
â”‚  â”‚   Weaviate, Pinecone, etc.)    â”‚                              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â”‚         â”‚                                                        â”‚
â”‚         â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚         â”‚                               â”‚                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  3. RETRIEVER                  â”‚  â”‚ User Query (Q)     â”‚      â”‚
â”‚  â”‚  - ConversÃ£o em embedding      â”‚  â”‚                    â”‚      â”‚
â”‚  â”‚  - Busca de similaridade       â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚  â”‚  - Top-K retrieval             â”‚         â”‚                    â”‚
â”‚  â”‚                                â”‚         â”‚                    â”‚
â”‚  â”‚  Tipos:                        â”‚         â”‚                    â”‚
â”‚  â”‚  â€¢ Dense (Embeddings)          â”‚         â”‚                    â”‚
â”‚  â”‚  â€¢ Sparse (BM25, TF-IDF)       â”‚         â”‚                    â”‚
â”‚  â”‚  â€¢ Hybrid (Ambos)              â”‚         â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚                    â”‚
â”‚         â”‚                                   â”‚                    â”‚
â”‚         â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”˜                    â”‚
â”‚         â”‚                                 â”‚                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                      â”‚
â”‚  â”‚  4. GERADOR (LLM)                   â”‚  â”‚                      â”‚
â”‚  â”‚  - Prompt engineering               â”‚  â”‚                      â”‚
â”‚  â”‚  - Context + Query concatenation    â”‚  â”‚                      â”‚
â”‚  â”‚  - GeraÃ§Ã£o de resposta              â”‚  â”‚                      â”‚
â”‚  â”‚                                     â”‚  â”‚                      â”‚
â”‚  â”‚  Input: [Context] + [Query]         â”‚  â”‚                      â”‚
â”‚  â”‚  Output: Resposta grounded          â”‚  â”‚                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                      â”‚
â”‚         â”‚                                 â”‚                      â”‚
â”‚         â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”˜                      â”‚
â”‚         â”‚                              â”‚                         â”‚
â”‚         â–¼                              â”‚                         â”‚
â”‚  [Resposta Final + Citations]          â”‚                         â”‚
â”‚                                        â”‚                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚
                                    User Interface
```

#### 1. **Indexador (Preprocessing & Ingestion)**

ResponsÃ¡vel por transformar documentos brutos em forma indexÃ¡vel:

**Passos:**
1. **Carregamento**: Ler documentos (PDFs, TXTs, JSONs, etc.)
2. **Parsing**: Extrair conteÃºdo estruturado
3. **Limpeza**: Remover ruÃ­do, normalizar texto
4. **Chunking**: Dividir em pedaÃ§os menores (256-1024 tokens tÃ­pico)
5. **Metadata ExtraÃ§Ã£o**: Tags, datas, autores, etc.
6. **Embedding**: Converter cada chunk em vetor semÃ¢ntico

#### 2. **Banco Vetorial (Vector Store)**

Armazena embeddings e permite busca por similaridade eficiente.

**FunÃ§Ãµes:**
- Armazenar milhÃµes/bilhÃµes de embeddings
- Buscas rÃ¡pidas de vizinhos mais prÃ³ximos (HNSW, IVF)
- Retornar metadados associados
- Suportar filtros por atributos

**Exemplos:** FAISS, Weaviate, Chroma, Milvus, Pinecone

#### 3. **Retriever (Mecanismo de Busca)**

Encontra os documentos mais relevantes para uma query.

**Tipos:**
- **Dense Retriever**: Usa embeddings semÃ¢nticos (BERT-based, Contriever, ColBERT)
- **Sparse Retriever**: Usa keywords (BM25, TF-IDF)
- **Hybrid Retriever**: Combina ambos (Melhor performance geral)

#### 4. **Gerador (LLM)**

LÃª o contexto recuperado e gera uma resposta.

**Entrada:** `[System Prompt] + [Context] + [User Query]`
**SaÃ­da:** Resposta contextualizada

### 2.2 Pipeline de Consulta: Passo a Passo

```
EXEMPLO: User pergunta "Qual Ã© a polÃ­tica de fÃ©rias da empresa?"

â”Œâ”€ PASSO 1: Query Ingestion â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input: "Qual Ã© a polÃ­tica de fÃ©rias da empresa?"â”‚
â”‚  AÃ§Ã£o: ValidaÃ§Ã£o e limpeza de texto              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€ PASSO 2: Query Embedding â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AÃ§Ã£o: Converter query em vetor (768-dim tÃ­pico) â”‚
â”‚  Modelo: Mesmo embedding model usado na index    â”‚
â”‚  Resultado: Query Vector Q                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€ PASSO 3: Busca de Similaridade â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AÃ§Ã£o: top-k busca (tÃ­pico k=5)                  â”‚
â”‚  MÃ©todo: Cosine similarity ou outro              â”‚
â”‚  Resultado: 5 chunks mais similares + scores     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€ PASSO 4: Ranking (Opcional) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AÃ§Ã£o: Re-rank usando cross-encoder              â”‚
â”‚  Objetivo: Melhorar ordem de relevÃ¢ncia          â”‚
â”‚  Resultado: Chunks reordenados                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€ PASSO 5: Context Assembly â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AÃ§Ã£o: Concatenar chunks em window               â”‚
â”‚  Formato: [Chunk 1]\n[Chunk 2]\n...              â”‚
â”‚  Resultado: Context string                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€ PASSO 6: Prompt Construction â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Template:                                      â”‚
â”‚  "Use o seguinte contexto para responder:"      â”‚
â”‚  [CONTEXT]                                      â”‚
â”‚  Pergunta: [QUERY]                              â”‚
â”‚  Responda brevemente.                           â”‚
â”‚                                                 â”‚
â”‚  Resultado: Full Prompt para LLM                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€ PASSO 7: LLM Inference â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLM lÃª prompt completo                         â”‚
â”‚  Gera resposta token-by-token                   â”‚
â”‚  Resposta: "A polÃ­tica de fÃ©rias Ã© 20 dias...   â”‚
â”‚             (conforme documento X)"             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€ PASSO 8: Post-Processing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AÃ§Ãµes:                                         â”‚
â”‚  â€¢ Extrair citations (quais chunks foram usados)â”‚
â”‚  â€¢ Validar factualidade (opcional)              â”‚
â”‚  â€¢ Formatar para user                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
               [Final Response + Citations]
```

### 2.3 VariaÃ§Ãµes Arquiteturais

#### RAG-Sequence vs. RAG-Token

Existem duas formulaÃ§Ãµes principais de RAG, conforme proposto por Lewis et al. (2020):

**RAG-Sequence (mais comum)**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Retriever retorna documentos UMA VEZ        â”‚
â”‚ Todos os tokens de resposta usam o mesmo    â”‚
â”‚ contexto recuperado                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ p(y|x) = Î£_z p(z|x) * p(y|z,x)              â”‚
â”‚          (mesmos z para todo y)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

EXEMPLO:
Query: "Como fazer bolo de chocolate?"
Retriever busca: [Receita de bolo, tÃ©cnicas culinÃ¡ria]
Generator usa AMBOS para gerar TODA resposta
```

**RAG-Token (mais flexÃ­vel mas custoso)**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Pode recuperar DIFERENTES documentos           â”‚
â”‚ para cada token da resposta                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ p(y|x) = Î _t Î£_z p(z|x,y_<t) * p(y_t|z,x,y_<t) â”‚
â”‚          (z diferente para cada y_t)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

EXEMPLO:
Resposta token-by-token com busca dinÃ¢mica:
Token 1 ("Misture") â†’ busca "tÃ©cnicas de mistura"
Token 2 ("ingredientes") â†’ busca "ingredientes bolo"
...
```

**ComparaÃ§Ã£o:**

|       Aspecto     |      RAG-Sequence     |             RAG-Token           |
|-------------------|-----------------------|---------------------------------|
| **Complexidade**  | Simples               | Complexa                        |
| **Custo**         | Uma busca/query       | Busca por token (~50-100+)      |
| **Flexibilidade** | Menos (contexto fixo) | Mais (contexto dinÃ¢mico)        |
| **Performance**   | Boa em geral          | Melhor em multi-hop mas custoso |
| **Uso PrÃ¡tico**   | Predominante          | Pesquisa/casos especÃ­ficos      |

### 2.4 Fluxo de Dados Detalhado

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              FLUXO DE DADOS EM RAG                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚ FASE 1: OFFLINE (PrÃ©-processamento)                          â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                         â”‚
â”‚ [Documentos Brutos]                                          â”‚
â”‚     â†“ (chunk_size=512, overlap=50)                           â”‚
â”‚ [Chunks: D1, D2, ..., DN]                                    â”‚
â”‚     â†“ (embedding_model="sentence-transformers/...")          â”‚
â”‚ [Embeddings: E1, E2, ..., EN] âˆˆ â„^768                        â”‚
â”‚     â†“ (indexing="hnsw")                                      â”‚
â”‚ [Vector Store com Ã­ndice]                                    â”‚
â”‚                                                              â”‚
â”‚ FASE 2: ONLINE (Tempo de InferÃªncia)                         â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                         â”‚
â”‚ [Query do usuÃ¡rio]                                           â”‚
â”‚     â†“ (same embedding model)                                 â”‚
â”‚ [Query Embedding] Q âˆˆ â„^768                                  â”‚
â”‚     â†“ (similarity search, k=5)                               â”‚
â”‚ [Top-5 chunks + cosine similarity scores]                    â”‚
â”‚ Exemplo: D_i: score=0.92, D_j: score=0.85, ...               â”‚
â”‚     â†“ (concatenate + truncate to max_context)                â”‚
â”‚ [Context String C]                                           â”‚
â”‚     â†“ (prompt template)                                      â”‚
â”‚ [Prompt P = System + Context + Query]                        â”‚
â”‚     â†“ (LLM forward pass)                                     â”‚
â”‚ [Token generation with attention over P]                     â”‚
â”‚     â†“ (extraction de citations)                              â”‚
â”‚ [Response R + Source References]                             â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## **MÃ“DULO 3: EMBEDDINGS E RECUPERAÃ‡ÃƒO**

### Objetivos de Aprendizado
- Entender o conceito e cÃ¡lculo de embeddings semÃ¢nticos
- Dominar mÃ©tricas de similaridade vetorial
- Conhecer bancos vetoriais populares e suas trade-offs
- Aprender otimizaÃ§Ãµes para preparar bases de conhecimento

### 3.1 Embeddings SemÃ¢nticos: Conceito Fundamental

**O que Ã© um embedding?**

Um embedding Ã© uma **representaÃ§Ã£o numÃ©rica densa** de texto que captura significado semÃ¢ntico em espaÃ§o vetorial de alta dimensÃ£o.

#### IntuiÃ§Ã£o MatemÃ¡tica

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TransformaÃ§Ã£o: Texto â†’ Vetor                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                        â”‚
â”‚ Entrada: "A inteligÃªncia artificial Ã© transformadora"  â”‚
â”‚                                                        â”‚
â”‚ â”Œâ”€ TokenizaÃ§Ã£o: ["A", "inteligÃªncia", "artificial",    â”‚
â”‚ â”‚                "Ã©", "transformadora"]                â”‚
â”‚ â”‚                                                      â”‚
â”‚ â”œâ”€ Embedding de cada token:                            â”‚
â”‚ â”‚  [0.2, -0.5, 0.8, ...] (e.g., 768-dim)               â”‚
â”‚ â”‚                                                      â”‚
â”‚ â”œâ”€ ComposiÃ§Ã£o (mean pooling + attention):              â”‚
â”‚ â”‚  Combinar embeddings de tokens                       â”‚
â”‚ â”‚                                                      â”‚
â”‚ â””â”€ SaÃ­da: Vetor final [0.15, -0.3, 0.5, ...] âˆˆ â„^768   â”‚
â”‚                                                        â”‚
â”‚ Propriedade Chave:                                     â”‚
â”‚ Textos semanticamente similares â†’ vetores prÃ³ximos     â”‚
â”‚ no espaÃ§o vetorial                                     â”‚
â”‚                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Como SÃ£o Criados?

Embeddings sÃ£o aprendidos atravÃ©s de **treinamento de modelos de linguagem**:

1. **Modelos de Base**: BERT, RoBERTa, etc.
   - Treinados com objetivo de prediÃ§Ã£o mascarada
   - Aprendem relaÃ§Ãµes semÃ¢nticas em linguagem natural

2. **Modelos Especializados em Similaridade**: Sentence-BERT, Contriever, ColBERT
   - Treinados com **contrastive learning**
   - Exemplos: Pares (query, doc_relevante, doc_irrelevante)
   - Objetivo: maximizar similiaridade entre pares relevantes

3. **Fine-tuning em DomÃ­nios**
   - Treinar embeddings em dados especÃ­ficos do domÃ­nio
   - Exemplo: embeddings legais, mÃ©dicos, etc.

**EquaÃ§Ã£o de Contrastive Learning (Simplified):**

```
Loss = -log[ exp(sim(q, d+) / Ï„) / 
            (exp(sim(q, d+) / Ï„) + Î£ exp(sim(q, d-) / Ï„)) ]

Onde:
- q: embedding da query
- d+: embedding do documento relevante
- d-: embedding de documentos irrelevantes
- Ï„: temperatura
- sim(): funÃ§Ã£o de similaridade (coseno, dot-product)
```

### 3.2 MÃ©tricas de Similaridade Vetorial

#### Cosine Similarity (Principal)

**DefiniÃ§Ã£o matemÃ¡tica:**

```
cos(A, B) = (A Â· B) / (||A|| Ã— ||B||)
          = Î£(A_i * B_i) / (âˆš(Î£ A_iÂ²) Ã— âˆš(Î£ B_iÂ²))
```

**Propriedades:**
- Retorna valores em [-1, 1]
- **1**: vetores idÃªnticos (mesma direÃ§Ã£o)
- **0**: ortogonais (sem correlaÃ§Ã£o)
- **-1**: opostos
- **Invariante a magnitude**: Dois vetores com mesma direÃ§Ã£o mas magnitudes diferentes tÃªm similaridade 1

**Por que Cosine para embeddings?**

Em espaÃ§os de alta dimensÃ£o (768-1536 dims tÃ­pico para LLM embeddings):
- DistÃ¢ncias Euclidianas tendem a convergir (todos pontos "longe" um do outro)
- Cosine similarity mede **Ã¢ngulo** entre vetores, nÃ£o magnitude
- Mais interpretÃ¡vel e estÃ¡vel

**Exemplo PrÃ¡tico:**

```python
# Dois embeddings 3D (simplificado)
A = [1, 0, 0]  # Representa conceito "gato"
B = [0.9, 0.2, 0.05]  # Representa conceito "felino" (similar)
C = [0, 1, 0]  # Representa conceito "mÃ¡quina" (diferente)

cos(A, B) = (1Ã—0.9 + 0Ã—0.2 + 0Ã—0.05) / (1 Ã— âˆš(0.81+0.04+0.0025))
          = 0.9 / 0.929 â‰ˆ 0.968 (Muito similar!)

cos(A, C) = (1Ã—0 + 0Ã—1 + 0Ã—0) / (1 Ã— 1)
          = 0 (Ortogonais - conceitos independentes)
```

#### Outras MÃ©tricas de Similaridade

|         MÃ©trica        |              FÃ³rmula            |               Quando Usar           |                      Pros/Cons               |
|------------------------|---------------------------------|-------------------------------------|----------------------------------------------|
| **Euclidean Distance** | âˆš(Î£(A_i - B_i)Â²)                | DistÃ¢ncias absolutas                | IntuiÃ§Ã£o geomÃ©trica; ineficiente em alta dim |
| **Manhattan Distance** | Î£\|A_i - B_i\|                  | EspaÃ§os estruturados                | Computacionalmente eficiente; menos preciso  |
| **Dot Product**        | A Â· B                           | Embeddings normalizados             | RÃ¡pido; precisa normalizaÃ§Ã£o prÃ©via          |
| **Hamming Distance**   | Contagem de diferenÃ§as (bits)   | Vetores binÃ¡rios/hash               | Muito rÃ¡pido; perda de informaÃ§Ã£o            |

#### Escolha PrÃ¡tica

Para RAG com embeddings de LLMs:
âœ… **Recomendado: Cosine Similarity**
- Embedding models modernos jÃ¡ estÃ£o normalizados (cos Ã© equivalente ao dot-product normalizado)
- Eficiente computacionalmente (FAISS, Pinecone otimizados para cosine)
- InterpretaÃ§Ã£o intuitiva

### 3.3 Bancos Vetoriais Populares

#### FAISS (Facebook AI Similarity Search)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CaracterÃ­sticas:                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Open-source, desenvolvido pelo Meta   â”‚
â”‚ â€¢ Muito rÃ¡pido (GPU acceleration)       â”‚
â”‚ â€¢ Suporta Ã­ndices: HNSW, IVF, LSH       â”‚
â”‚ â€¢ Memory-efficient com quantizaÃ§Ã£o      â”‚
â”‚ â€¢ Sem suporte nativo a metadados        â”‚
â”‚                                         â”‚
â”‚ Ideal Para: Busca em larga escala,      â”‚
â”‚ prototipagem local, pesquisa            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

# Exemplo de uso:
import faiss
import numpy as np

# Criar Ã­ndice (1M embeddings de 768-dim)
embeddings = np.random.rand(1000000, 768).astype('float32')
index = faiss.IndexFlatL2(768)  # Euclidean
index.add(embeddings)

# Busca
query = np.random.rand(1, 768).astype('float32')
distances, indices = index.search(query, k=5)

# Para performance, usar Ã­ndices estruturados:
index = faiss.IndexIVFFlat(faiss.IndexFlatL2(768), 768, 100)
index.train(embeddings)
index.add(embeddings)
```

#### Chroma

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CaracterÃ­sticas:                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Projeto recente, Python-native        â”‚
â”‚ â€¢ Foco em developer experience          â”‚
â”‚ â€¢ Armazena embeddings + metadados       â”‚
â”‚ â€¢ IntegraÃ§Ã£o natural com LangChain      â”‚
â”‚ â€¢ Suporta persistÃªncia local (SQLite)   â”‚
â”‚ â€¢ Escalabilidade: atÃ© milhÃµes de docs   â”‚
â”‚                                         â”‚
â”‚ Ideal Para: Prototipagem rÃ¡pida,        â”‚
â”‚ RAG para PDFs/docs, pequeno-mÃ©dio scale â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

# Exemplo de uso:
import chromadb
from chromadb.config import Settings

client = chromadb.Client(Settings(
    chroma_db_impl="duckdb+parquet",
    persist_directory="/path/to/data"
))

# Criar collection
collection = client.create_collection(name="documents")

# Adicionar documentos com embeddings
collection.add(
    ids=["doc1", "doc2"],
    embeddings=[[...], [...]],  # auto-generated se embedding_function providida
    metadatas=[{"source": "pdf1"}, {"source": "pdf2"}],
    documents=["ConteÃºdo do doc1", "ConteÃºdo do doc2"]
)

# Buscar
results = collection.query(
    query_embeddings=[[...]],
    n_results=5,
    where={"source": {"$eq": "pdf1"}}  # Filtros!
)
```

#### Weaviate

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CaracterÃ­sticas:                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Cloud-native, arquitetura distribuÃ­da â”‚
â”‚ â€¢ Knowledge Graph + Vector Search       â”‚
â”‚ â€¢ GraphQL API poderosa                  â”‚
â”‚ â€¢ Suporta metadados e relacionamentos   â”‚
â”‚ â€¢ Enterprise-ready com RBAC             â”‚
â”‚ â€¢ Multi-modal (texto, imagem)           â”‚
â”‚                                         â”‚
â”‚ Ideal Para: Sistemas corporativos,      â”‚
â”‚ dados complexos, escalas Enterprise     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

# Exemplo de uso (Python):
import weaviate
from weaviate.connect import ConnectionParams

client = weaviate.connect_to_local(port=6379)

# Define schema
client.collections.create(
    name="Document",
    vectorizer_config=weaviate.config.Configure.Vectorizer.text2vec_huggingface(),
    properties=[
        weaviate.config.Property(
            name="title", data_type=weaviate.config.DataType.TEXT
        ),
        weaviate.config.Property(
            name="content", data_type=weaviate.config.DataType.TEXT
        ),
        weaviate.config.Property(
            name="source", data_type=weaviate.config.DataType.TEXT
        ),
    ]
)

# Adicionar dados
collection = client.collections.get("Document")
collection.data.insert({
    "title": "AI 101",
    "content": "Artificial Intelligence fundamentals...",
    "source": "Wikipedia"
})

# Buscar
results = collection.query.hybrid(
    query="O que Ã© IA?",
    limit=5,
    where=weaviate.query.Filter.by_property("source").equal("Wikipedia")
)
```

#### Milvus

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CaracterÃ­sticas:                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Open-source, cloud-native (CNCF)       â”‚
â”‚ â€¢ EscalÃ¡vel para bilhÃµes de vetores      â”‚
â”‚ â€¢ Kubernetes-ready                       â”‚
â”‚ â€¢ Clustering e replicaÃ§Ã£o automÃ¡tica     â”‚
â”‚ â€¢ Suporta mÃºltiplos tipos de Ã­ndices     â”‚
â”‚ â€¢ Benchmarks de alta performance         â”‚
â”‚                                          â”‚
â”‚ Ideal Para: Escala web-scale,            â”‚
â”‚ ambientes containerizados, performance   â”‚
â”‚ crÃ­tica                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

# Exemplo (simplificado):
from pymilvus import connections, Collection, FieldSchema, \
    CollectionSchema, DataType, create_index

connections.connect("default", host="localhost", port="19530")

# Define schema
fields = [
    FieldSchema(name="id", dtype=DataType.INT64, is_primary=True),
    FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=768),
    FieldSchema(name="source", dtype=DataType.VARCHAR, max_length=100),
]
schema = CollectionSchema(fields)

# Create collection
collection = Collection("documents", schema)

# Index para performance
index_params = {
    "index_type": "HNSW",
    "metric_type": "L2",
    "params": {"M": 8, "efConstruction": 200}
}
collection.create_index("embedding", index_params)

# Buscar
collection.load()  # Carregar em memÃ³ria
search_params = {"metric_type": "L2", "params": {"ef": 200}}
results = collection.search(query_vectors, "embedding", search_params, limit=5)
```

#### Pinecone

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CaracterÃ­sticas:                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Fully Managed (SaaS)                   â”‚
â”‚ â€¢ Sem overhead operacional               â”‚
â”‚ â€¢ Escalabilidade automÃ¡tica              â”‚
â”‚ â€¢ Pod-based pricing (previsÃ­vel)         â”‚
â”‚ â€¢ Metadata filtering, sparse-dense hybridâ”‚
â”‚ â€¢ Enterprise SLA                         â”‚
â”‚ â€¢ Downside: Vendor lock-in, custo        â”‚
â”‚                                          â”‚
â”‚ Ideal Para: Empresas que querem          â”‚
â”‚ zero ops, scale automÃ¡tica, SLA garantidaâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

# Exemplo de uso:
import pinecone

pinecone.init(api_key="xxx", environment="us-west4-gcp")

# Create index
pinecone.create_index(
    name="documents",
    dimension=768,
    metric="cosine",
    pod_type="p1"
)

index = pinecone.Index("documents")

# Upsert vectors
index.upsert(vectors=[
    ("doc-1", [0.1, 0.2, ..., 0.8], {"source": "pdf1", "page": 1}),
    ("doc-2", [0.2, 0.3, ..., 0.7], {"source": "pdf2", "page": 2}),
])

# Query
results = index.query(
    vector=[0.15, 0.25, ..., 0.75],
    top_k=5,
    filter={"source": {"$eq": "pdf1"}}  # Metadata filter
)
```

#### ComparaÃ§Ã£o Resumida

|       CritÃ©rio     |       FAISS     |    Chroma     |  Weaviate  |   Milvus   |    Pinecone    |
|--------------------|-----------------|---------------|------------|------------|----------------|
| **Setup**          | Simples         | Muito Simples | Complexo   | Moderado   | Trivial (SaaS) |
| **Escalabilidade** | AtÃ© 1B (single) | AtÃ© 10M+      | 100B+      | 1B+        | Ilimitada      |
| **Metadados**      | NÃ£o nativo      | Sim           | Sim, Graph | Sim        | Sim            |
| **Custo**          | GrÃ¡tis          | GrÃ¡tis        | GrÃ¡tis     | GrÃ¡tis     | Pago           |
| **Ops**            | Manual          | MÃ­nima        | Kubernetes | Kubernetes | Zero           |
| **Melhor Para**    | Pesquisa, local | Prototipo RAG | Enterprise | Escala     | Simplici       |

### 3.4 PreparaÃ§Ã£o e OtimizaÃ§Ã£o da Base de Conhecimento

#### EstratÃ©gias de Chunking

A qualidade da recuperaÃ§Ã£o depende muito de como os documentos sÃ£o divididos.

**Problema Fundamental:**
```
Chunks PEQUENOS demais:
â”œâ”€ âœ“ FÃ¡cil encontrar exatamente o relevante
â”œâ”€ âœ— Perdem contexto
â””â”€ âœ— FragmentaÃ§Ã£o semÃ¢ntica

Chunks GRANDES demais:
â”œâ”€ âœ“ MantÃªm contexto rico
â”œâ”€ âœ— Dificuldade em recuperaÃ§Ã£o precisa
â”œâ”€ âœ— Excede janela de contexto do LLM
â””â”€ âœ— RuÃ­do (muito texto irrelevante junto)

Optimal Ã© BALANCEADO (350-512 tokens tÃ­pico)
```

**EstratÃ©gias Principais:**

1. **Fixed-Size Chunking** (Simples)
```python
def chunk_text(text, chunk_size=512, overlap=50):
    chunks = []
    for i in range(0, len(text), chunk_size - overlap):
        chunks.append(text[i:i+chunk_size])
    return chunks
```
Pros: Simples, rÃ¡pido
Cons: Pode quebrar no meio de sentenÃ§as/conceitos

2. **Sentence-Based Chunking** (Melhor)
```python
import nltk
from nltk.tokenize import sent_tokenize

def chunk_by_sentences(text, target_chunk_size=512):
    sentences = sent_tokenize(text)
    chunks = []
    current_chunk = ""
    
    for sent in sentences:
        if len(current_chunk) + len(sent) < target_chunk_size:
            current_chunk += " " + sent
        else:
            chunks.append(current_chunk)
            current_chunk = sent
    
    if current_chunk:
        chunks.append(current_chunk)
    
    return chunks
```
Pros: Respeita limites semÃ¢nticos
Cons: Chunks podem variar em tamanho

3. **Recursive Splitting** (Recomendado)
```python
# Tenta manter estrutura hierÃ¡rquica
# Split por: "\n\n" (parÃ¡grafos) 
#  â†’ "\n" (linhas)
#  â†’ "." (sentenÃ§as)
#  â†’ " " (palavras)

from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=512,
    chunk_overlap=50,
    separators=["\n\n", "\n", ".", " ", ""]
)
chunks = splitter.split_text(text)
```
Pros: MantÃ©m estrutura; lida com vÃ¡rios formatos
Cons: Mais lento

4. **Hierarchical Chunking** (AvanÃ§ado)
```
Documento
 â”œâ”€ CapÃ­tulo 1 (chunk grande = section)
 â”‚  â”œâ”€ SeÃ§Ã£o 1.1 (chunk mÃ©dio)
 â”‚  â”‚  â”œâ”€ ParÃ¡grafo 1.1.a (chunk pequeno)
 â”‚  â”‚  â””â”€ ParÃ¡grafo 1.1.b
 â”‚  â””â”€ SeÃ§Ã£o 1.2
 â””â”€ CapÃ­tulo 2
```
Permite multi-hop retrieval (buscar no nivel apropriado)

#### Enriquecimento de Metadados

```python
# Cada chunk deve ter:
chunk_with_metadata = {
    "content": "texto do chunk...",
    "source": "documento.pdf",
    "page": 3,
    "section": "CapÃ­tulo 2: IntroduÃ§Ã£o",
    "chunk_id": "chunk_001",
    "timestamp": "2024-01-15",
    "tags": ["IA", "RAG", "NLP"],
    "summary": "Resumo em 1-2 sentenÃ§as do chunk"  # Ãštil para retriever
}
```

BenefÃ­cios:
- Filtros mais precisos (`where source=="contrato.pdf"`)
- Rastreabilidade de citations
- Ajuste de ranking por metadados

#### OtimizaÃ§Ãµes para Retriever

1. **Query Expansion**
```
Query original: "Como fazer login?"

Expandido:
â”œâ”€ "authentication process"
â”œâ”€ "user sign in"
â”œâ”€ "account access"
â””â”€ "password reset"

Buscar com todas as variaÃ§Ãµes â†’ mais recalls
```

2. **Contextual Window para Chunks**
```
Problema: Um chunk sozinho pode ser ambÃ­guo
SoluÃ§Ã£o: Recuperar chunk + contexto antes/depois

# PadrÃ£o "Parent Document":
- Index com chunks pequenos (256 tokens)
- Mas retornar chunks MAIORES que os indexados
```

3. **Embedding Caching**
```python
# Calcular embedding uma vez, reusar mÃºltiplas vezes
embeddings_cache = {}

def get_embedding(text, model):
    if text in embeddings_cache:
        return embeddings_cache[text]
    embedding = model.encode(text)
    embeddings_cache[text] = embedding
    return embedding
```

---

## **MÃ“DULO 4: INTEGRAÃ‡ÃƒO COM MODELOS DE LINGUAGEM**

### Objetivos de Aprendizado
- Entender como LLMs consomem contexto recuperado
- Dominar estratÃ©gias de prompt engineering para RAG
- Aprender tÃ©cnicas de controle de contexto (chunking, windowing)
- Implementar estratÃ©gias anti-alucinaÃ§Ã£o

### 4.1 Como LLMs Consomem Contexto Recuperado

#### Arquitetura Interna de AtenÃ§Ã£o

Um LLM baseia-se em **Transformer architecture** com mecanismo de atenÃ§Ã£o:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Fluxo de Processamento no Transformer          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                        â”‚
â”‚ INPUT: [CONTEXT_TOKEN_1, ..., CONTEXT_TOKEN_m,         â”‚
â”‚         QUERY_TOKEN_1, ..., QUERY_TOKEN_n]             â”‚
â”‚                                                        â”‚
â”‚ â†“ Embedding Layer                                      â”‚
â”‚                                                        â”‚
â”‚ [e1, e2, ..., em, q1, q2, ..., qn] âˆˆ â„^d               â”‚
â”‚                                                        â”‚
â”‚ â†“ Positional Encoding (mantÃ©m ordem dos tokens)        â”‚
â”‚                                                        â”‚
â”‚ â†“ Multi-Head Self-Attention                            â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚   â”‚ AtenciÃ³n = softmax((QÂ·K^T)/âˆšd) Â· V  â”‚              â”‚
â”‚   â”‚                                     â”‚              â”‚
â”‚   â”‚ Cada token "attende" (pesa)         â”‚              â”‚
â”‚   â”‚ para todos outros tokens            â”‚              â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚   â†’ Resultado: Cada token sabe qual contexto           â”‚
â”‚     Ã© relevante para ele                               â”‚
â”‚                                                        â”‚
â”‚ â†“ Feed-Forward Networks                                â”‚
â”‚                                                        â”‚
â”‚ â†“ Layer Normalization & Residual Connections           â”‚
â”‚                                                        â”‚
â”‚ [repeat para 12-96 layers dependendo modelo]           â”‚
â”‚                                                        â”‚
â”‚ â†“ Output Layer (LM Head)                               â”‚
â”‚                                                        â”‚
â”‚ [p(token_next | todos_tokens_anteriores)]              â”‚
â”‚ âˆˆ [0,1]^vocab_size                                     â”‚
â”‚                                                        â”‚
â”‚ OUTPUT: PrÃ³ximo token (via argmax ou sampling)         â”‚
â”‚                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Como Contexto Influencia GeraÃ§Ã£o

**Empiricamente observado (papers):**

1. **Primeira parte do contexto tem mais influÃªncia** (primacy bias)
   - Tokens iniciais recebem mais atenÃ§Ã£o
   - âš ï¸ ImplicaÃ§Ã£o: Colocar informaÃ§Ã£o crÃ­tica no inÃ­cio do contexto

2. **Tokens duplicados em contexto amplificam sua influÃªncia**
   - Se informaÃ§Ã£o aparece mÃºltiplas vezes â†’ mais peso
   - âœ“ Usar quando quer enforce certa resposta

3. **Contexto muito longo diminui efetividade** (lost in the middle)
   - LLMs tendem a ignorar informaÃ§Ã£o no meio de contextos longos
   - Ã“timo em ~500-1000 tokens; decresce depois

4. **Formato e estrutura do contexto importam**
   - Markdown estruturado: melhor
   - XML tags: bom para separar seÃ§Ãµes
   - Plain text: funciona mas menos eficiente

### 4.2 EstratÃ©gias de Prompt Engineering para RAG

#### Template Base Efetivo

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              PROMPT TEMPLATE RECOMENDADO                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚ SYSTEM:                                                 â”‚
â”‚ "VocÃª Ã© um assistente Ãºtil e preciso. Responda          â”‚
â”‚  perguntas baseado EXCLUSIVAMENTE no contexto           â”‚
â”‚  fornecido. Se a resposta nÃ£o estiver no contexto,      â”‚
â”‚  diga 'NÃ£o encontrei informaÃ§Ã£o relevante'."            â”‚
â”‚                                                         â”‚
â”‚ USER:                                                   â”‚
â”‚ "Contexto:                                              â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                â”‚
â”‚ {CONTEXT_HERE}                                          â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                â”‚
â”‚                                                         â”‚
â”‚ Pergunta: {QUERY}                                       â”‚
â”‚                                                         â”‚
â”‚ Responda em 2-3 sentenÃ§as. Cite suas fontes."           â”‚
â”‚                                                         â”‚
â”‚ ASSISTANT:                                              â”‚
â”‚ [LLM gera resposta]                                     â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Tecnicas AvanÃ§adas de Prompt Engineering

**1. Chain-of-Thought (CoT) para RAG**
```
SYSTEM: "VocÃª Ã© um assistente analÃ­tico que raciocina 
passo a passo."

USER: "{CONTEXT}

Pergunta: {QUERY}

Pense passo a passo:
1. Quais informaÃ§Ãµes no contexto sÃ£o relevantes?
2. Que conexÃµes posso fazer?
3. Qual Ã© minha resposta final?"

Efeito: LLM gera explicitamente seu raciocÃ­nio
       â†’ Mais accurate, hallucina menos
       â†’ Pode validar raciocÃ­nio antes de responder
```

**2. Few-Shot Prompting**
```
SYSTEM: "Responda perguntas baseado no contexto.
Aqui estÃ£o exemplos:"

EXEMPLO 1:
Contexto: "Python Ã© uma linguagem de programaÃ§Ã£o..."
Pergunta: "O que Ã© Python?"
Resposta: "Python Ã© uma linguagem de programaÃ§Ã£o 
conforme afirmado no documento."

EXEMPLO 2:
[... mais exemplos ...]

USER: "{ATUAL_CONTEXTO}
Pergunta: {ATUAL_QUERY}"

Efeito: LLM tem "padrÃ£o" para seguir
       â†’ Mais consistent, evita desvios
```

**3. Structured Output**
```
USER: "{CONTEXT}

Pergunta: {QUERY}

Responda em JSON:
{
  'answer': 'sua resposta aqui',
  'confidence': 0-100,
  'sources': ['chunk1', 'chunk2'],
  'reasoning': 'por que chegou nesta resposta'
}"

Efeito: Output estruturado
       â†’ FÃ¡cil de parse automaticamente
       â†’ Includes confidence/reasoning
```

### 4.3 Chunking e Windowing AvanÃ§ado

#### Problema de Context Window

```
LLM Context Window: 4096 tokens (tÃ­pico)

DistribuiÃ§Ã£o:
â”œâ”€ System Prompt: 100 tokens
â”œâ”€ Contexto Recuperado: 3000 tokens
â”œâ”€ Query: 50 tokens
â””â”€ GeraÃ§Ã£o (espaÃ§o disponÃ­vel): 946 tokens

PROBLEMA: Resposta pode ser truncada!
```

#### EstratÃ©gia 1: Sliding Window
```python
def apply_sliding_window(
    context_chunks: List[str],
    max_context_tokens: int = 2000,
    overlap_ratio: float = 0.1
):
    """
    Em vez de usar todos chunks, usa 'janela' que
    se move sobre o contexto.
    """
    window_tokens = 0
    window = []
    
    for chunk in context_chunks:
        tokens = len(chunk.split())
        
        if window_tokens + tokens > max_context_tokens:
            # Estourou limite, para de adicionar
            break
        
        window.append(chunk)
        window_tokens += tokens
    
    return " ".join(window)

# Uso:
limited_context = apply_sliding_window(
    top_k_chunks,
    max_context_tokens=2000
)
```

#### EstratÃ©gia 2: Hierarchical Summarization
```python
def summarize_context(
    chunks: List[str],
    llm,
    target_tokens: int = 1500
):
    """
    Se contexto Ã© muito grande, resumir chunks
    antes de passar para generator.
    """
    summaries = []
    
    for chunk in chunks:
        summary_prompt = f"""
        Resuma o seguinte em 2-3 sentenÃ§as:
        
        {chunk}
        """
        summary = llm.generate(summary_prompt)
        summaries.append(summary)
    
    full_context = "\n".join(summaries)
    
    if len(full_context.split()) > target_tokens:
        # Ainda muito grande, sumarizar novamente
        return summarize_context(
            [full_context],
            llm,
            target_tokens
        )
    
    return full_context
```

#### EstratÃ©gia 3: Reranking Inteligente
```python
from sentence_transformers import CrossEncoder

def rerank_by_relevance(
    query: str,
    chunks: List[str],
    reranker_model: str = "cross-encoder/mmarco-mMiniLMv2-L12-H384-v1",
    top_k: int = 5,
    max_context_tokens: int = 2000
):
    """
    Use cross-encoder para re-ranquear chunks
    baseado em relevÃ¢ncia PAIRWISE.
    
    Cross-encoder Ã© mais preciso que dense retrieval
    mas mais custoso (O(n) similarity scores).
    """
    reranker = CrossEncoder(reranker_model)
    
    # Score cada chunk contra query
    pairs = [[query, chunk] for chunk in chunks]
    scores = reranker.predict(pairs)
    
    # Sort por score (descendente)
    ranked_chunks = sorted(
        zip(chunks, scores),
        key=lambda x: x[1],
        reverse=True
    )
    
    # Selecionar top-K atÃ© limite de tokens
    selected = []
    token_count = 0
    
    for chunk, score in ranked_chunks:
        chunk_tokens = len(chunk.split())
        if token_count + chunk_tokens > max_context_tokens:
            break
        selected.append(chunk)
        token_count += chunk_tokens
    
    return selected
```

### 4.4 Controle de AlucinaÃ§Ã£o

#### Tipos de AlucinaÃ§Ã£o em RAG

```
1. ALUCINAÃ‡ÃƒO INTRÃN SECA (nÃ£o relacionada ao contexto)
   Query: "Quantos satÃ©lites tem Marte?"
   Contexto: (sobre VÃªnus)
   SaÃ­da: "Marte tem 12 satÃ©lites" (inventado)
   
2. ALUCINAÃ‡ÃƒO CONTEXTUAL (distorÃ§Ã£o do contexto)
   Contexto: "A populaÃ§Ã£o cresceu 10% no ano X"
   SaÃ­da: "A populaÃ§Ã£o dobrou" (exagerado)
   
3. ALUCINAÃ‡ÃƒO DE CITAÃ‡ÃƒO (fake attribution)
   SaÃ­da: "Conforme documento Z... (nunca mencionado)"
   
4. ALUCINAÃ‡ÃƒO COMPOSITIVA (combinaÃ§Ã£o errada)
   Contexto: "A Ã© B" e "C Ã© D"
   SaÃ­da: "A Ã© D" (falsa combinaÃ§Ã£o)
```

#### TÃ©cnicas de MitigaÃ§Ã£o

**1. Confidence Scoring**
```python
def score_hallucination_risk(
    generated_text: str,
    context: str,
    llm
) -> float:
    """
    Score de 0-1 indicando risco de alucinaÃ§Ã£o.
    """
    verify_prompt = f"""
    Dado o contexto abaixo:
    
    CONTEXTO:
    {context}
    
    A seguinte declaraÃ§Ã£o Ã© suportada pelo contexto?
    
    DECLARAÃ‡ÃƒO:
    {generated_text}
    
    Responda: SIM, NÃƒO, ou PARCIAL
    
    Se PARCIAL, explique qual parte Ã© suportada.
    """
    
    verification = llm.generate(verify_prompt)
    
    if "SIM" in verification.upper():
        return 0.0  # Sem risco
    elif "PARCIAL" in verification.upper():
        return 0.5  # Risco moderado
    else:
        return 1.0  # Alto risco
```

**2. Grounding Enforcement via System Prompt**
```python
system_prompt = """
REGRAS RIGOROSAS:
1. Responda EXCLUSIVAMENTE baseado no contexto.
2. Se informaÃ§Ã£o nÃ£o estiver no contexto, 
   diga: "NÃ£o encontrei informaÃ§Ã£o no contexto."
3. Nunca invente ou suponha fatos.
4. Cite a seÃ§Ã£o especÃ­fica de cada afirmaÃ§Ã£o.
5. Se tiver dÃºvida, peÃ§a confirmaÃ§Ã£o.

Formato de resposta:
- DeclaraÃ§Ã£o: [statement]
- Fonte: [SeÃ§Ã£o X do documento Y]
- ConfianÃ§a: [ALTA/MÃ‰DIA/BAIXA]
"""
```

**3. Retrieval-Verification Loop**
```python
def rag_with_verification(
    query: str,
    retriever,
    generator_llm,
    verifier_llm,
    max_iterations: int = 3
) -> str:
    """
    Loop iterativo: Retrieve â†’ Generate â†’ Verify â†’ 
    Se falha verificaÃ§Ã£o, retrieve mais contexto
    """
    
    for iteration in range(max_iterations):
        # Retrieve
        context = retriever.retrieve(query)
        
        # Generate
        response = generator_llm.generate(
            context=context,
            query=query
        )
        
        # Verify
        verify_prompt = f"""
        O seguinte contexto suporta esta resposta?
        
        CONTEXTO: {context}
        RESPOSTA: {response}
        
        Responda: VÃLIDO ou INVÃLIDO
        Se INVÃLIDO, explique por quÃª.
        """
        
        verification = verifier_llm.generate(verify_prompt)
        
        if "VÃLIDO" in verification:
            return response  # âœ“ Response is grounded
        
        # Se invÃ¡lido, tentar com mais contexto
        print(f"IteraÃ§Ã£o {iteration+1}: VerificaÃ§Ã£o falhou. "
              f"RazÃ£o: {verification}")
        
        # Aumentar k para prÃ³xima retrieval
        retriever.k += 2
    
    return response  # Retornar mesmo que nÃ£o verificado
```

---

## **MÃ“DULO 5: IMPLEMENTAÃ‡ÃƒO PRÃTICA**

### Objetivos de Aprendizado
- Implementar pipeline RAG funcional de ponta a ponta
- Integrar com LangChain e LlamaIndex
- Trabalhar com APIs de LLM (OpenAI, Anthropic, Mistral)
- Construir sistema completo com persistÃªncia

### 5.1 RAG Completo com LangChain

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# IMPLEMENTAÃ‡ÃƒO COMPLETA DE RAG COM LANGCHAIN
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# 1. INSTALAÃ‡Ã•ES
"""
pip install langchain langchain-community
pip install langchain-openai  # ou outro provider
pip install chroma  # Vector store
pip install pypdf  # Para ler PDFs
pip install python-dotenv  # Para gerenciar env vars
"""

# 2. IMPORTS
import os
from dotenv import load_dotenv
from typing import List

from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import ChatPromptTemplate

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# PASSO 1: CARREGAR DOCUMENTOS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def load_documents(pdf_paths: List[str]) -> List:
    """
    Carrega PDFs e extrai conteÃºdo.
    """
    documents = []
    
    for pdf_path in pdf_paths:
        print(f"Carregando: {pdf_path}")
        loader = PyPDFLoader(pdf_path)
        docs = loader.load()
        documents.extend(docs)
    
    print(f"Total de documentos carregados: {len(documents)}")
    return documents

# Uso:
pdf_files = ["documento1.pdf", "documento2.pdf"]
raw_documents = load_documents(pdf_files)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# PASSO 2: CHUNKING (DIVISÃƒO EM PEQUENOS PEDAÃ‡OS)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def split_documents(documents, chunk_size: int = 512, 
                   chunk_overlap: int = 50):
    """
    Divide documentos em chunks menores mantendo contexto.
    """
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=["\n\n", "\n", ".", " ", ""]
    )
    
    chunks = text_splitter.split_documents(documents)
    print(f"Total de chunks criados: {len(chunks)}")
    
    return chunks

chunks = split_documents(raw_documents)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# PASSO 3: CRIAR EMBEDDINGS E STORE VETORIAL
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def create_vector_store(
    chunks,
    embedding_model: str = "text-embedding-3-small",
    persist_dir: str = "./chroma_db"
):
    """
    Cria embeddings e armazena em Chroma.
    """
    
    # Usar embeddings OpenAI (ou outro modelo)
    embeddings = OpenAIEmbeddings(model=embedding_model)
    
    # Criar vector store com persistÃªncia
    vector_store = Chroma.from_documents(
        documents=chunks,
        embedding=embeddings,
        persist_directory=persist_dir
    )
    
    print(f"Vector store criado em: {persist_dir}")
    return vector_store

vector_store = create_vector_store(chunks)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# PASSO 4: CONFIGURAR RETRIEVER
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def setup_retriever(vector_store, search_type: str = "similarity", 
                   k: int = 5):
    """
    Cria retriever a partir do vector store.
    """
    
    retriever = vector_store.as_retriever(
        search_type=search_type,  # "similarity" ou "similarity_score_threshold"
        search_kwargs={
            "k": k,  # Top-k resultados
            # "score_threshold": 0.5  # Opcional: min score
        }
    )
    
    return retriever

retriever = setup_retriever(vector_store, k=5)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# PASSO 5: CONFIGURAR LLM GENERATOR
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def setup_llm(model: str = "gpt-4", temperature: float = 0):
    """
    Cria instÃ¢ncia do LLM para geraÃ§Ã£o.
    """
    
    llm = ChatOpenAI(
        model_name=model,
        temperature=temperature,  # 0 = determinÃ­stico
        max_tokens=1024
    )
    
    return llm

llm = setup_llm()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# PASSO 6: CRIAR PROMPT CUSTOMIZADO
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def create_rag_prompt():
    """
    Cria template de prompt para RAG.
    """
    
    prompt_template = ChatPromptTemplate.from_messages([
        ("system", """VocÃª Ã© um assistente Ãºtil especializado 
em responder perguntas baseado em documentos fornecidos.

REGRAS:
1. Responda EXCLUSIVAMENTE baseado no contexto fornecido
2. Se a informaÃ§Ã£o nÃ£o estiver no contexto, 
   diga: "NÃ£o encontrei informaÃ§Ã£o relevante nos documentos"
3. Cite as fontes de suas respostas
4. Seja preciso e conciso"""),
        
        ("human", """Contexto dos documentos:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
{context}
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Pergunta do usuÃ¡rio: {question}

Responda em 3-4 sentenÃ§a, citando as fontes.""")
    ])
    
    return prompt_template

prompt = create_rag_prompt()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# PASSO 7: CRIAR CHAIN RAG
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

from langchain.chains import RetrievalQA

def create_rag_chain(llm, retriever, prompt):
    """
    Combina retriever + LLM em um chain.
    """
    
    rag_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",  # Concatena todos docs
        retriever=retriever,
        return_source_documents=True,  # Retorna chunks usados
        chain_type_kwargs={
            "prompt": prompt
        }
    )
    
    return rag_chain

rag_chain = create_rag_chain(llm, retriever, prompt)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# PASSO 8: USAR O SISTEMA RAG
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def query_rag(rag_chain, query: str) -> dict:
    """
    Execute uma query no sistema RAG.
    """
    
    print(f"\nğŸ” Pergunta: {query}\n")
    
    result = rag_chain({"query": query})
    
    print(f"ğŸ“ Resposta:\n{result['result']}\n")
    
    print("ğŸ“š Documentos utilizados:")
    for i, doc in enumerate(result['source_documents'], 1):
        print(f"  {i}. PÃ¡gina {doc.metadata.get('page', 'N/A')} - "
              f"{doc.metadata.get('source', 'Desconhecida')}")
    
    return result

# Exemplos de uso
queries = [
    "Qual Ã© a polÃ­tica de fÃ©rias da empresa?",
    "Quais sÃ£o os benefÃ­cios oferecidos?",
    "Como solicitar um dia de folga?"
]

for query in queries:
    query_rag(rag_chain, query)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# PASSO 9: MELHORAMENTOS OPCIONAIS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Recarregar vector store persistente (prÃ³xima execuÃ§Ã£o)
vector_store_loaded = Chroma(
    persist_directory="./chroma_db",
    embedding_function=OpenAIEmbeddings()
)

# Adicionar documentos novos
new_docs = load_documents(["novo_documento.pdf"])
new_chunks = split_documents(new_docs)
vector_store_loaded.add_documents(new_chunks)

# Usar Reranker para melhorar resultados
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder

base_retriever = vector_store_loaded.as_retriever(search_kwargs={"k": 10})

compressor = CrossEncoderReranker(
    model=HuggingFaceCrossEncoder(
        model_name="cross-encoder/ms-marco-MiniLM-L-12-v2"
    )
)

compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=base_retriever
)

# Usar compression_retriever em vez de base_retriever
```

### 5.2 RAG com LlamaIndex (Alternativa Modular)

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# IMPLEMENTAÃ‡ÃƒO COM LLAMAINDEX (MAIS ESPECIALIZAADO PARA RAG)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
pip install llama-index llama-index-llms-openai
pip install llama-index-embeddings-openai
pip install llama-index-readers-file
pip install pypdf
"""

import os
from llama_index.core import (
    VectorStoreIndex,
    SimpleDirectoryReader,
    Settings,
    StorageContext
)
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.core.query_engines import RetrieverQueryEngine
from llama_index.core.postprocessor import SimilarityPostprocessor
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.core.node_parser import SimpleNodeParser

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# PASSO 1: CONFIGURAÃ‡ÃƒO GLOBAL
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Settings.llm = OpenAI(model="gpt-4", temperature=0)
Settings.embed_model = OpenAIEmbedding(model="text-embedding-3-small")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# PASSO 2: CARREGAR DOCUMENTOS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

documents = SimpleDirectoryReader("./documents").load_data()
print(f"Documentos carregados: {len(documents)}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# PASSO 3: CRIAR ÃNDICE VETORIAL
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Com persistÃªncia
import chromadb

chroma_client = chromadb.PersistentClient(path="./chroma_data")
chroma_collection = chroma_client.get_or_create_collection("documents")

vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
storage_context = StorageContext.from_defaults(vector_store=vector_store)

# Criar Ã­ndice
index = VectorStoreIndex.from_documents(
    documents,
    storage_context=storage_context,
    show_progress=True
)

print("Ãndice criado com sucesso!")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# PASSO 4: CONFIGURAR RETRIEVER COM OPÃ‡Ã•ES AVANÃ‡ADAS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

retriever = VectorIndexRetriever(
    index=index,
    similarity_top_k=5,  # Top-5 resultados
)

# Adicionar post-processador para re-ranking
postprocessor = SimilarityPostprocessor(similarity_cutoff=0.5)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# PASSO 5: CRIAR QUERY ENGINE COM PROMPT CUSTOMIZADO
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

from llama_index.core.prompts import PromptTemplate

qa_prompt_str = """Contexto das informaÃ§Ãµes:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
{context_str}
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Pergunta: {query_str}

InstruÃ§Ãµes:
1. Responda EXCLUSIVAMENTE baseado no contexto
2. Se nÃ£o souber, diga "NÃ£o encontrei informaÃ§Ã£o"
3. Cite as fontes
4. Responda em 2-3 sentenÃ§as"""

qa_prompt = PromptTemplate(qa_prompt_str)

# Criar query engine
query_engine = RetrieverQueryEngine(
    retriever=retriever,
    node_postprocessors=[postprocessor],
    text_qa_template=qa_prompt
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# PASSO 6: EXECUTAR QUERIES
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

response = query_engine.query("Qual Ã© a polÃ­tica de fÃ©rias?")

print(f"Resposta: {response}")
print(f"\nFontes utilizadas:")
for node in response.source_nodes:
    print(f"  - {node.metadata.get('file_name', 'Unknown')}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# PASSO 7: INTEGRAÃ‡ÃƒO COM LANGCHAIN (OPTIONAL)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

from langchain.retrievers import LlamaIndexRetriever
from langchain.chains import RetrievalQA
from langchain_openai import ChatOpenAI

# Converter retriever LlamaIndex para LangChain
langchain_retriever = LlamaIndexRetriever(retriever)

# Usar em LangChain chain
langchain_llm = ChatOpenAI(model="gpt-4")
langchain_chain = RetrievalQA.from_chain_type(
    llm=langchain_llm,
    chain_type="stuff",
    retriever=langchain_retriever
)

# Usar como antes
result = langchain_chain({"query": "Qual Ã© a polÃ­tica de fÃ©rias?"})
print(result['result'])
```

### 5.3 Exemplo PrÃ¡tico: Chatbot sobre PDFs

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CHATBOT RAG INTERATIVO SOBRE MÃšLTIPLOS PDFS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import os
import streamlit as st
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import ChatPromptTemplate

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# UI COM STREAMLIT
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

st.set_page_config(page_title="RAG Chatbot", layout="wide")
st.title("ğŸ“š Assistente de Documentos com RAG")

# Sidebar para upload
with st.sidebar:
    st.header("Upload de Documentos")
    uploaded_files = st.file_uploader(
        "Selecione PDFs",
        type=['pdf'],
        accept_multiple_files=True
    )
    
    process_button = st.button("Processar Documentos")
    
    # ParÃ¢metros
    chunk_size = st.slider("Tamanho do Chunk", 256, 1024, 512)
    k_results = st.slider("Top-K resultados", 1, 10, 5)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# PROCESSAMENTO DE ARQUIVOS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@st.cache_resource
def process_documents(uploaded_files_list, chunk_sz):
    """Process uploads e cria vector store (cached)"""
    
    documents = []
    
    for uploaded_file in uploaded_files_list:
        # Salvar temporÃ¡rio
        with open(f"temp_{uploaded_file.name}", "wb") as f:
            f.write(uploaded_file.getbuffer())
        
        # Carregar
        loader = PyPDFLoader(f"temp_{uploaded_file.name}")
        docs = loader.load()
        
        # Adicionar metadata
        for doc in docs:
            doc.metadata['source'] = uploaded_file.name
        
        documents.extend(docs)
    
    # Chunk
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_sz,
        chunk_overlap=50
    )
    chunks = splitter.split_documents(documents)
    
    # Vector store
    embeddings = OpenAIEmbeddings()
    vector_store = Chroma.from_documents(
        chunks,
        embeddings
    )
    
    # Limpeza
    for uploaded_file in uploaded_files_list:
        os.remove(f"temp_{uploaded_file.name}")
    
    return vector_store

# Processar ao clicar
if process_button and uploaded_files:
    with st.spinner("Processando documentos..."):
        vector_store = process_documents(uploaded_files, chunk_size)
    st.success("âœ… Documentos processados!")
    st.session_state.vector_store = vector_store

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CHAT INTERFACE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if 'vector_store' in st.session_state:
    # Criar chain
    retriever = st.session_state.vector_store.as_retriever(
        search_kwargs={"k": k_results}
    )
    
    llm = ChatOpenAI(model="gpt-4", temperature=0)
    
    prompt_template = ChatPromptTemplate.from_messages([
        ("system", "VocÃª Ã© um assistente Ãºtil. "
                   "Responda baseado no contexto fornecido."),
        ("human", "{context}\n\nPergunta: {question}")
    ])
    
    rag_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": prompt_template}
    )
    
    # Chat history
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    # Display chat history
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.write(message["content"])
    
    # User input
    if query := st.chat_input("FaÃ§a uma pergunta sobre os documentos..."):
        st.session_state.messages.append({"role": "user", "content": query})
        
        with st.chat_message("user"):
            st.write(query)
        
        # Generate response
        with st.spinner("Procurando informaÃ§Ã£o..."):
            result = rag_chain({"query": query})
        
        response_text = result['result']
        st.session_state.messages.append({
            "role": "assistant",
            "content": response_text
        })
        
        with st.chat_message("assistant"):
            st.write(response_text)
            
            # Mostrar fontes
            with st.expander("ğŸ“„ Ver fontes"):
                for doc in result['source_documents']:
                    st.write(f"**{doc.metadata.get('source', 'Unknown')}**")
                    st.write(doc.page_content[:500] + "...")

else:
    st.info("ğŸ‘ˆ FaÃ§a upload de PDFs no painel lateral para comeÃ§ar!")
```

---

## **MÃ“DULO 6: AVALIAÃ‡ÃƒO E MÃ‰TRICAS**

### Objetivos de Aprendizado
- Compreender mÃ©tricas de recuperaÃ§Ã£o (Recall, Precision, MRR)
- Aprender mÃ©tricas de geraÃ§Ã£o (Factualidade, RelevÃ¢ncia)
- Implementar frameworks de avaliaÃ§Ã£o automatizados
- Construir pipelines de testes para RAG

### 6.1 MÃ©tricas de RecuperaÃ§Ã£o

#### Recall@K (RelembranÃ§a)

**DefiniÃ§Ã£o:**
```
Recall@K = (NÃºmero de documentos relevantes no top-K) 
           / (Total de documentos relevantes no corpus)
```

Responde: "De TODOS os documentos relevantes que existem, 
quantos foram encontrados?"

**Exemplo:**
```
Corpus: 1000 documentos
Documentos relevantes TOTAIS: 50

Query retorna top-5 com 3 relevantes
Recall@5 = 3 / 50 = 0.06 (6%)

Mesma query, top-100 com 30 relevantes
Recall@100 = 30 / 50 = 0.60 (60%)

â†’ Alto recall@100 mas baixo recall@5
â†’ Retriever encontra coisas, mas precisa buscar muito
```

**Quando Usar:**
- âœ… Importa encontrar TUDO (busca legal, anÃ¡lise completa)
- âŒ NÃ£o importa quando tempo Ã© crÃ­tico (latÃªncia)

#### Precision@K (PrecisÃ£o)

**DefiniÃ§Ã£o:**
```
Precision@K = (NÃºmero de documentos relevantes no top-K)
              / (Total de documentos retornados no top-K)
```

Responda: "Dos documentos que retornamos, quantos sÃ£o realmente relevantes?"

**Exemplo:**
```
Retorno top-5 documentos
Dos 5, 3 sÃ£o relevantes
Precision@5 = 3 / 5 = 0.60 (60%)

â†’ 60% dos meus retornos sÃ£o bons
â†’ Menos falsos positivos
```

**Quando Usar:**
- âœ… Importa evitar ruÃ­do (buscas corporativas, suporte)
- âŒ NÃ£o importa omitir alguns resultados

#### Mean Reciprocal Rank (MRR)

**DefiniÃ§Ã£o:**
```
MRR = (1/N) * Î£ (1 / rank_i)

Onde rank_i Ã© a posiÃ§Ã£o do PRIMEIRO documento relevante
para query i
```

Responde: "Em mÃ©dia, em que posiÃ§Ã£o o primeiro resultado 
relevante aparece?"

**Exemplo:**
```
Query 1: Primeiro relevante na posiÃ§Ã£o 2 â†’ 1/2 = 0.5
Query 2: Primeiro relevante na posiÃ§Ã£o 1 â†’ 1/1 = 1.0
Query 3: Nenhum relevante (rank=âˆ) â†’ 1/âˆ â‰ˆ 0

MRR = (0.5 + 1.0 + 0) / 3 = 0.5

â†’ Em mÃ©dia, primeira coisa relevante Ã© achada na posiÃ§Ã£o 2
â†’ Boa para cenÃ¡rios onde SÃ“ A PRIMEIRA resposta importa
```

**Quando Usar:**
- âœ… InformaÃ§Ã£o-seeking (usuÃ¡rio quer resposta rÃ¡pida)
- âœ… QA systems

#### NDCG (Normalized Discounted Cumulative Gain)

**Conceito:**
```
NDCG mede qualidade do RANKING, nÃ£o sÃ³ presenÃ§a/ausÃªncia
Documents podem ser "relevantes" em graus (0-5 stars)

FÃ³rmula:
DCG@K = Î£ (rel_i / log2(i+1))

Onde rel_i Ã© relevÃ¢ncia do documento na posiÃ§Ã£o i

NDCG@K = DCG@K / IDCG@K
(Normalizado pelo melhor ranking possÃ­vel)
```

**Exemplo:**
```
Ideal ranking: [5, 5, 4, 3, 2] (melhor possÃ­vel)
IDCG = 5/log(2) + 5/log(3) + 4/log(4) + 3/log(5) + 2/log(6)
     â‰ˆ 5 + 3.15 + 2 + 1.29 + 0.73 = 12.17

Meu retriever retorna: [5, 3, 4, 5, 1]
DCG = 5/log(2) + 3/log(3) + 4/log(4) + 5/log(5) + 1/log(6)
    â‰ˆ 5 + 1.89 + 2 + 2.15 + 0.15 = 11.19

NDCG@5 = 11.19 / 12.17 = 0.92 (92%)
```

#### Tabela Comparativa

|      MÃ©trica    |          O que mede      |    Valor Ideal  |        CenÃ¡rio        |
|-----------------|--------------------------|-----------------|-----------------------|
| **Precision@5** | Pureza dos top-5         | 1.0 (100%)      | Buscas corporativas   |
| **Recall@10**   | Cobertura dos relevantes | 1.0 (100%)      | AnÃ¡lise legal         |
| **MRR**         | Rank do primeiro bom     | 1.0 (posiÃ§Ã£o 1) | Google-like search    |
| **NDCG@10**     | Qualidade do ranking     | 1.0 (perfeito)  | Mecanismos de ranking |

### 6.2 MÃ©tricas de GeraÃ§Ã£o

#### Faithfulness (Factualidade)

**DefiniÃ§Ã£o:**
Mede se a resposta gerada Ã© suportada pelo contexto recuperado.

**ImplementaÃ§Ã£o com LLM-based Metric:**

```python
from ragas.metrics import Faithfulness
from datasets import Dataset

# Dataset estruturado
eval_dataset = Dataset.from_dict({
    "question": ["O que Ã© IA?", "Quando foi fundada?"],
    "contexts": [
        [["IA Ã© um campo da computaÃ§Ã£o..."]],
        [["A empresa foi fundada em 2020..."]]
    ],
    "answer": [
        "IA (InteligÃªncia Artificial) Ã© um ramo da computaÃ§Ã£o.",
        "A empresa foi fundada em 2020."
    ]
})

# MÃ©trica
faithfulness = Faithfulness()

# Avaliar
scores = faithfulness.score(eval_dataset)
print(f"Faithfulness: {scores['faithfulness']}")  # 0-1
```

**MÃ©todo Manual:**
```python
def evaluate_faithfulness(
    answer: str,
    context: str,
    llm
) -> float:
    """
    Score 0-1 de quÃ£o factual a resposta Ã©.
    """
    
    prompt = f"""
    Dado o CONTEXTO abaixo:
    
    CONTEXTO:
    {context}
    
    A seguinte RESPOSTA Ã© suportada pelos fatos no contexto?
    
    RESPOSTA:
    {answer}
    
    Responda como JSON:
    {{"suportada": true/false, "score": 0-1, "razao": "..."}}
    """
    
    # LLM avalia
    eval_response = llm.generate(prompt)
    
    import json
    result = json.loads(eval_response)
    
    return result["score"]
```

#### Answer Relevance

**DefiniÃ§Ã£o:**
Mede se a resposta responde Ã  pergunta original.

```python
from ragas.metrics import AnswerRelevancy

# Usar RAGAS framework (recomendado)
relevancy = AnswerRelevancy()

# Se a pergunta era "Qual Ã© a capital?"
# E resposta Ã© "A capital Ã© Paris."
# Answer Relevance seria high (1.0)

# Se resposta fosse "Paris fica na Europa"
# Answer Relevance seria lower (pode ser 0.7)

scores = relevancy.score(eval_dataset)
```

#### Answer Correctness (AcurÃ¡cia)

**DefiniÃ§Ã£o:**
Compara resposta gerada com resposta esperada (ground truth).

```python
from rouge_score import rouge_scorer
from bert_score import score as bert_score

def evaluate_answer_correctness(
    generated_answer: str,
    ground_truth_answer: str,
    metric: str = "rouge"
) -> float:
    """
    Compara resposta gerada com esperada.
    """
    
    if metric == "rouge":
        scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
        scores = scorer.score(ground_truth_answer, generated_answer)
        return scores['rougeL'].fmeasure
    
    elif metric == "bert":
        # BERTScore usa similaridade semÃ¢ntica
        precision, recall, f1 = bert_score(
            [generated_answer],
            [ground_truth_answer],
            lang="en"
        )
        return f1[0].item()  # F1 score
    
    elif metric == "exact_match":
        return 1.0 if generated_answer.lower() == ground_truth_answer.lower() else 0.0

# Exemplo:
accuracy = evaluate_answer_correctness(
    "A capital da FranÃ§a Ã© Paris",
    "Paris Ã© a capital da FranÃ§a"
)
print(f"Accuracy: {accuracy}")  # ~ 0.95
```

### 6.3 Framework de AvaliaÃ§Ã£o Completo

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PIPELINE DE AVALIAÃ‡ÃƒO AUTOMATIZADO PARA RAG
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from typing import List, Dict
import json
from datetime import datetime
from tqdm import tqdm

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. DEFINIR DATASET DE AVALIAÃ‡ÃƒO
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class EvalDataset:
    """Dataset estruturado para avaliaÃ§Ã£o."""
    
    def __init__(self):
        self.samples = []
    
    def add_sample(self, question: str, expected_answer: str, 
                   relevant_docs: List[str], tags: List[str] = None):
        """Adicionar sample de teste."""
        self.samples.append({
            "question": question,
            "expected_answer": expected_answer,
            "relevant_docs": relevant_docs,
            "tags": tags or []
        })
    
    def load_from_json(self, filepath: str):
        """Carregar dataset de arquivo JSON."""
        with open(filepath) as f:
            self.samples = json.load(f)
    
    def save_to_json(self, filepath: str):
        """Salvar dataset."""
        with open(filepath, 'w') as f:
            json.dump(self.samples, f, indent=2)

# Exemplo de dataset
eval_dataset = EvalDataset()
eval_dataset.add_sample(
    question="Qual Ã© a polÃ­tica de fÃ©rias?",
    expected_answer="20 dias Ãºteis por ano",
    relevant_docs=["CapÃ­tulo 3 - BenefÃ­cios", "SeÃ§Ã£o 3.2 - FÃ©rias"],
    tags=["benefits", "hr_policy"]
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. IMPLEMENTAR MÃ‰TRICAS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class RAGEvaluator:
    """Classe para avaliar sistema RAG."""
    
    def __init__(self, rag_chain, eval_llm):
        self.rag_chain = rag_chain
        self.eval_llm = eval_llm
    
    def eval_retrieval_metrics(self, 
                              retrieved_docs: List[str],
                              relevant_docs: List[str]) -> Dict:
        """Calcular mÃ©tricas de retrieval."""
        
        # Precision@K
        k = len(retrieved_docs)
        relevant_retrieved = sum(1 for doc in retrieved_docs 
                                if doc in relevant_docs)
        precision = relevant_retrieved / k if k > 0 else 0
        
        # Recall@K
        recall = relevant_retrieved / len(relevant_docs) \
            if len(relevant_docs) > 0 else 0
        
        # F1
        f1 = 2 * (precision * recall) / (precision + recall) \
            if (precision + recall) > 0 else 0
        
        return {
            "precision": precision,
            "recall": recall,
            "f1": f1,
            "retrieved_count": k,
            "relevant_count": len(relevant_docs)
        }
    
    def eval_generation_metrics(self,
                               generated_answer: str,
                               expected_answer: str,
                               context: str) -> Dict:
        """Calcular mÃ©tricas de generation."""
        
        # Faithfulness (LLM-based)
        faithful_prompt = f"""
        Contexto: {context}
        
        Resposta: {generated_answer}
        
        A resposta Ã© suportada pelo contexto? (0-1)
        Responda apenas com um nÃºmero.
        """
        
        faithfulness_response = self.eval_llm.generate(faithful_prompt)
        faithfulness = float(faithfulness_response) / 100 \
            if "%" in faithfulness_response else float(faithfulness_response)
        
        # Answer Relevance (LLM-based)
        relevance_prompt = f"""
        Pergunta: {expected_answer}
        
        Resposta: {generated_answer}
        
        A resposta aborda a pergunta? (0-1)
        """
        
        relevance_response = self.eval_llm.generate(relevance_prompt)
        relevance = float(relevance_response) / 100 \
            if "%" in relevance_response else float(relevance_response)
        
        return {
            "faithfulness": faithfulness,
            "relevance": relevance,
            "avg_generation_score": (faithfulness + relevance) / 2
        }
    
    def evaluate_sample(self, sample: Dict) -> Dict:
        """Avaliar um sample completo."""
        
        question = sample["question"]
        expected = sample["expected_answer"]
        relevant_docs_info = sample["relevant_docs"]
        
        # Executar RAG
        result = self.rag_chain({"query": question})
        generated_answer = result["result"]
        retrieved_docs = [doc.page_content for doc 
                         in result.get("source_documents", [])]
        context = "\n".join(retrieved_docs)
        
        # Avaliar retrieval
        retrieval_metrics = self.eval_retrieval_metrics(
            retrieved_docs,
            relevant_docs_info
        )
        
        # Avaliar generation
        generation_metrics = self.eval_generation_metrics(
            generated_answer,
            expected,
            context
        )
        
        return {
            "question": question,
            "generated_answer": generated_answer,
            "expected_answer": expected,
            "retrieval_metrics": retrieval_metrics,
            "generation_metrics": generation_metrics,
            "combined_score": (
                retrieval_metrics["f1"] * 0.4 +
                generation_metrics["avg_generation_score"] * 0.6
            )
        }
    
    def evaluate_dataset(self, eval_dataset: EvalDataset) -> Dict:
        """Avaliar dataset inteiro."""
        
        results = {
            "timestamp": datetime.now().isoformat(),
            "total_samples": len(eval_dataset.samples),
            "samples": [],
            "aggregated_metrics": {}
        }
        
        # Avaliar cada sample
        for sample in tqdm(eval_dataset.samples, desc="Avaliando"):
            result = self.evaluate_sample(sample)
            results["samples"].append(result)
        
        # Agregar mÃ©tricas
        results["aggregated_metrics"] = self._aggregate_results(results["samples"])
        
        return results
    
    def _aggregate_results(self, samples: List[Dict]) -> Dict:
        """Agregrar resultados de mÃºltiplos samples."""
        
        avg_precision = sum(s["retrieval_metrics"]["precision"] 
                           for s in samples) / len(samples)
        avg_recall = sum(s["retrieval_metrics"]["recall"] 
                        for s in samples) / len(samples)
        avg_f1 = sum(s["retrieval_metrics"]["f1"] 
                    for s in samples) / len(samples)
        
        avg_faithfulness = sum(s["generation_metrics"]["faithfulness"] 
                              for s in samples) / len(samples)
        avg_relevance = sum(s["generation_metrics"]["relevance"] 
                           for s in samples) / len(samples)
        
        avg_combined = sum(s["combined_score"] 
                          for s in samples) / len(samples)
        
        return {
            "retrieval": {
                "avg_precision": avg_precision,
                "avg_recall": avg_recall,
                "avg_f1": avg_f1
            },
            "generation": {
                "avg_faithfulness": avg_faithfulness,
                "avg_relevance": avg_relevance
            },
            "overall_score": avg_combined
        }

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. USAR O EVALUADOR
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

from langchain_openai import ChatOpenAI

evaluator = RAGEvaluator(rag_chain=rag_chain, 
                         eval_llm=ChatOpenAI(model="gpt-4"))

# Avaliar
results = evaluator.evaluate_dataset(eval_dataset)

# Salvar resultados
with open("eval_results.json", "w") as f:
    json.dump(results, f, indent=2)

# Imprimir resumo
print(f"\n{'='*50}")
print(f"EVALUATION RESULTS")
print(f"{'='*50}")
print(f"\nSamples avaliados: {results['total_samples']}")
print(f"\nRetrievalMetrics:")
for metric, value in results["aggregated_metrics"]["retrieval"].items():
    print(f"  {metric}: {value:.3f}")

print(f"\nGeneration Metrics:")
for metric, value in results["aggregated_metrics"]["generation"].items():
    print(f"  {metric}: {value:.3f}")

print(f"\nOverall Score: {results['aggregated_metrics']['overall_score']:.3f}")
```

---

## **MÃ“DULO 7: CASOS DE USO E MELHORES PRÃTICAS**

### Objetivos de Aprendizado
- Explorar aplicaÃ§Ãµes reais de RAG em diversos setores
- Aprender padrÃµes de deployment em produÃ§Ã£o
- Implementar manutenÃ§Ã£o e atualizaÃ§Ã£o de Ã­ndices
- Comparar RAG com fine-tuning e LoRA

### 7.1 AplicaÃ§Ãµes Reais em Empresas

#### Case Study 1: Busca Corporativa (Enterprise Search)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PROBLEMA EMPRESARIAL                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ 10.000+ documentos (PDFs, Wikis)    â”‚
â”‚ â€¢ FuncionÃ¡rios perdem tempo buscando  â”‚
â”‚ â€¢ InformaÃ§Ã£o desatualizada            â”‚
â”‚ â€¢ DifÃ­cil encontrar contexto          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   SOLUÃ‡ÃƒO RAG                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Indexar TODOS os docs corporativos â”‚
â”‚    (intranets, polÃ­ticas, manuais)    â”‚
â”‚                                       â”‚
â”‚ 2. Retriever: Busca semÃ¢ntica         â”‚
â”‚    â†’ "Como solicitar fÃ©rias?"         â”‚
â”‚    â† Retorna seÃ§Ãµes relevantes        â”‚
â”‚                                       â”‚
â”‚ 3. Generator: LLM gera resposta       â”‚
â”‚    â†’ "VocÃª pode solicitar por..."     â”‚
â”‚    â† Com citation exata               â”‚
â”‚                                       â”‚
â”‚ 4. Update automÃ¡tico: Novos docs      â”‚
â”‚    â†’ Sync com Google Drive/Sharepoint â”‚
â”‚                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

BENEFÃCIOS MENSURADOS:
â€¢ ReduÃ§Ã£o de 40% em tempo de busca
â€¢ 30% menos suporte tickets
â€¢ 95% satisfaÃ§Ã£o do usuÃ¡rio
```

#### Case Study 2: Suporte TÃ©cnico Automatizado

```
PIPELINE:
Ticket do cliente 
    â†“
Query: "Produto nÃ£o liga"
    â†“
RAG Retrieves:
  â€¢ FAQ #123: "Verificar bateria"
  â€¢ Troubleshooting Guide
  â€¢ Common Issues Database
    â†“
LLM Gera resposta:
  "Obrigado por entrar em contato.
   Por favor, verifique:
   1. Bateria carregada (FAQ #123)
   2. BotÃ£o liga/desliga (TG-45)
   ..."
    â†“
Resposta enviada em <30s (vs. 2h manual)

ROI: 60% reduÃ§Ã£o de custos operacionais
```

#### Case Study 3: AnÃ¡lise de Contratos Legais

```
CENÃRIO:
Departamento legal tem 1000+ contratos
Precisa encontrar clÃ¡usulas especÃ­ficas rapidamente

IMPLEMENTAÃ‡ÃƒO RAG:
â”œâ”€ Chunking semÃ¢ntico (por clÃ¡usula)
â”œâ”€ Metadados: cliente, data, tipo contrato
â”œâ”€ Retriever: Dense + Sparse (Hybrid)
â”œâ”€ LLM: Especializado em anÃ¡lise legal
â”‚   (Fine-tuned ou prompt engenheirado)
â””â”€ Output: ExtraÃ§Ã£o estruturada (JSON)

EXEMPLO QUERY:
"Encontre todas as clÃ¡usulas de indenizaÃ§Ã£o 
com limite > $1M desde 2022"

RAG RESPOSTA:
{
  "matching_clauses": [
    {
      "contract": "ACC-2023-0451.pdf",
      "clause": "Section 4.2 - Indemnification",
      "excerpt": "...",
      "limit": "$2.5M",
      "effective_date": "2023-01-15"
    },
    ...
  ],
  "total_matches": 47
}

IMPACTO: AceleraÃ§Ã£o 10x em anÃ¡lise contratual
```

#### Case Study 4: Healthcare - AssistÃªncia DiagnÃ³stica

```
SISTEMA: Assistente de DiagnÃ³stico com RAG

KNOWLEDGE BASE:
â”œâ”€ Literatura mÃ©dica (PubMed papers)
â”œâ”€ Protocolos clÃ­nicos
â”œâ”€ HistÃ³rico de pacientes (anonymized)
â”œâ”€ Guidelines de tratamento
â””â”€ Estudos de caso

WORKFLOW:
1. MÃ©dico: "Paciente com febre 39Â°C + tosse"
   
2. RAG Retrieves:
   â€¢ Papers sobre infecÃ§Ãµes respiratÃ³rias
   â€¢ Protocolos de avaliaÃ§Ã£o
   â€¢ HistÃ³ricos similares
   
3. LLM Gera anÃ¡lise:
   "Baseado na literatura:
    - Considerar pneumonia viral/bacteriana
    - Recomenda-se teste COVID-19 (Guideline X)
    - Se prescrever antibiÃ³tico, considerar..."
    
4. Citations permitem verificaÃ§Ã£o
   mÃ©dica

COMPLIANCE:
âœ“ HIPAA compliance (dados locais)
âœ“ Rastreabilidade (citations)
âœ“ Sem diagnosis automÃ¡tica (auxilia MD)
```

### 7.2 EstratÃ©gias de Deployment em ProduÃ§Ã£o

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DEPLOYMENT PRODUCTION-READY DE RAG
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import logging
from typing import Optional
import asyncio
from datetime import datetime
import redis

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. LOGGING E MONITORING
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('rag_production.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class RAGMetricsCollector:
    """Coleta mÃ©tricas de performance em produÃ§Ã£o."""
    
    def __init__(self, redis_client):
        self.redis = redis_client
    
    def log_query(self, query_id: str, query_text: str, 
                 response_time: float, num_docs_retrieved: int,
                 model_used: str):
        """Registrar query e mÃ©tricas associadas."""
        
        metrics = {
            "query_id": query_id,
            "query_text": query_text,
            "response_time_ms": response_time,
            "docs_retrieved": num_docs_retrieved,
            "model": model_used,
            "timestamp": datetime.now().isoformat()
        }
        
        # Salvar em Redis para anÃ¡lise
        self.redis.rpush("rag_metrics", str(metrics))
        logger.info(f"Query logged: {query_id}, Time: {response_time}ms")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. CACHING PARA PERFORMANCE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class CachedRAG:
    """RAG com caching de queries comuns."""
    
    def __init__(self, rag_chain, cache_ttl_seconds: int = 3600):
        self.rag_chain = rag_chain
        self.cache_ttl = cache_ttl_seconds
        self.cache = redis.Redis(host='localhost', port=6379)
    
    def query(self, question: str) -> str:
        """Query com cache."""
        
        # Normalizar para cache
        cache_key = f"rag:{question.lower().strip()}"
        
        # Checar cache
        cached = self.cache.get(cache_key)
        if cached:
            logger.info(f"Cache hit for: {question}")
            return cached.decode()
        
        # Cache miss - execute RAG
        logger.info(f"Cache miss for: {question}")
        result = self.rag_chain({"query": question})
        answer = result["result"]
        
        # Salvar no cache
        self.cache.setex(cache_key, self.cache_ttl, answer)
        
        return answer

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. RETRY LOGIC E ERROR HANDLING
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

import time
from tenacity import retry, stop_after_attempt, wait_exponential

class RobustRAG:
    """RAG com retry automÃ¡tico e tratamento de erro."""
    
    def __init__(self, rag_chain, max_retries: int = 3):
        self.rag_chain = rag_chain
        self.max_retries = max_retries
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10)
    )
    def _execute_rag(self, question: str):
        """Execute com retry automÃ¡tico."""
        return self.rag_chain({"query": question})
    
    def query(self, question: str) -> Optional[dict]:
        """Query robusta."""
        try:
            result = self._execute_rag(question)
            logger.info(f"Query successful: {question[:50]}...")
            return result
        except Exception as e:
            logger.error(f"Query failed after {self.max_retries} retries: {e}")
            return {
                "result": "Desculpe, nÃ£o foi possÃ­vel processar sua pergunta. "
                         "Por favor, tente novamente mais tarde.",
                "error": str(e)
            }

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. AUTO-REFRESH DE ÃNDICE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

import schedule
from pathlib import Path

class AutoRefreshingRAG:
    """RAG com Ã­ndice que se atualiza automaticamente."""
    
    def __init__(self, rag_chain, documents_directory: str,
                 refresh_interval_hours: int = 24):
        self.rag_chain = rag_chain
        self.docs_dir = Path(documents_directory)
        self.refresh_interval = refresh_interval_hours
        self.last_refresh = None
    
    def check_for_new_documents(self) -> list:
        """Verificar novos documentos."""
        
        new_docs = []
        current_time = datetime.now()
        
        for doc_file in self.docs_dir.glob("*.pdf"):
            mod_time = datetime.fromtimestamp(doc_file.stat().st_mtime)
            
            if self.last_refresh is None or mod_time > self.last_refresh:
                new_docs.append(doc_file)
        
        return new_docs
    
    async def auto_refresh(self):
        """Refresh periÃ³dico do Ã­ndice."""
        
        while True:
            await asyncio.sleep(self.refresh_interval * 3600)
            
            new_docs = self.check_for_new_documents()
            
            if new_docs:
                logger.info(f"Refreshing index with {len(new_docs)} new docs")
                self._refresh_index(new_docs)
                self.last_refresh = datetime.now()
    
    def _refresh_index(self, new_docs: list):
        """Atualizar Ã­ndice com novos documentos."""
        # Implementar lÃ³gica de refresh especÃ­fica
        pass

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. RATE LIMITING E QUOTAS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

from functools import wraps

class RateLimitedRAG:
    """RAG com rate limiting por user."""
    
    def __init__(self, rag_chain, 
                 queries_per_minute: int = 30):
        self.rag_chain = rag_chain
        self.qpm = queries_per_minute
        self.query_history = {}  # user_id -> [timestamps]
    
    def is_allowed(self, user_id: str) -> bool:
        """Verificar se user atingiu limite."""
        
        now = time.time()
        one_minute_ago = now - 60
        
        # Limpar queries antigas
        if user_id not in self.query_history:
            self.query_history[user_id] = []
        
        recent_queries = [ts for ts in self.query_history[user_id]
                         if ts > one_minute_ago]
        self.query_history[user_id] = recent_queries
        
        # Checar limite
        return len(recent_queries) < self.qpm
    
    def query(self, user_id: str, question: str) -> dict:
        """Query com rate limit."""
        
        if not self.is_allowed(user_id):
            logger.warning(f"Rate limit exceeded for user: {user_id}")
            return {
                "error": "Rate limit exceeded. "
                        f"MÃ¡ximo {self.qpm} queries por minuto.",
                "retry_after_seconds": 60
            }
        
        # Log do timestamp
        self.query_history[user_id].append(time.time())
        
        # Execute
        return self.rag_chain({"query": question})
```

### 7.3 ManutenÃ§Ã£o do Ãndice

#### EstratÃ©gia de Versionamento

```python
class VersionedVectorStore:
    """Vector store com versionamento."""
    
    def __init__(self, base_path: str = "./vector_stores"):
        self.base_path = Path(base_path)
        self.current_version = self._get_latest_version()
    
    def _get_latest_version(self) -> int:
        """Encontrar Ãºltima versÃ£o."""
        versions = [
            int(d.name.split('_')[1])
            for d in self.base_path.glob("store_*")
        ]
        return max(versions) if versions else 0
    
    def create_new_version(self, documents: list) -> str:
        """Criar nova versÃ£o do store."""
        
        new_version = self.current_version + 1
        version_path = self.base_path / f"store_v{new_version}"
        version_path.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"Creating new vector store version: {new_version}")
        
        # Criar store
        embeddings = OpenAIEmbeddings()
        vector_store = Chroma.from_documents(
            documents=documents,
            embedding=embeddings,
            persist_directory=str(version_path)
        )
        
        # Registrar versÃ£o
        metadata = {
            "version": new_version,
            "created_at": datetime.now().isoformat(),
            "num_docs": len(documents),
            "status": "active"
        }
        
        with open(version_path / "metadata.json", "w") as f:
            json.dump(metadata, f)
        
        self.current_version = new_version
        return str(version_path)
    
    def rollback_to_version(self, version: int):
        """Voltar a versÃ£o anterior."""
        
        version_path = self.base_path / f"store_v{version}"
        
        if not version_path.exists():
            raise ValueError(f"Version {version} not found")
        
        logger.info(f"Rolling back to version: {version}")
        self.current_version = version
```

### 7.4 RAG vs Fine-tuning vs LoRA

#### ComparaÃ§Ã£o Detalhada

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              RAG vs Fine-Tuning vs LoRA                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Aspecto         â”‚ RAG              â”‚ Fine-Tuning   â”‚ LoRA    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CONCEITO        â”‚ Retrieve + Gen   â”‚ Train params  â”‚ Train   â”‚
â”‚                 â”‚                  â”‚ completos     â”‚ subset  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Custo Treino    â”‚ $0 (indexaÃ§Ã£o)   â”‚ $$$ alto      â”‚ $ baixo â”‚
â”‚                 â”‚                  â”‚               â”‚         â”‚
â”‚ Tempo Treino    â”‚ Horas (index)    â”‚ Dias/Semanas  â”‚ Horas   â”‚
â”‚                 â”‚                  â”‚               â”‚         â”‚
â”‚ MemÃ³ria Treino  â”‚ GPU 24GB         â”‚ GPU 40-80GB   â”‚ GPU 16GBâ”‚
â”‚                 â”‚                  â”‚               â”‚         â”‚
â”‚ LatÃªncia        â”‚ ~500ms (busca)   â”‚ <100ms        â”‚ <100ms  â”‚
â”‚                 â”‚                  â”‚               â”‚         â”‚
â”‚ AtualizaÃ§Ã£o     â”‚ Trivial (add doc)â”‚ Retreinar     â”‚ Retrein â”‚
â”‚                 â”‚                  â”‚               â”‚ (rÃ¡pido)â”‚
â”‚                 â”‚                  â”‚               â”‚         â”‚
â”‚ Conhecimento    â”‚ DinÃ¢mico (sempre â”‚ EstÃ¡tico      â”‚ EstÃ¡tic â”‚
â”‚                 â”‚ atualizado)      â”‚ (fixed)       â”‚ o       â”‚
â”‚                 â”‚                  â”‚               â”‚         â”‚
â”‚ Interpretab.    â”‚ Alto (cita doc)  â”‚ Baixa (black  â”‚ Baixa   â”‚
â”‚                 â”‚                  â”‚ box)          â”‚         â”‚
â”‚                 â”‚                  â”‚               â”‚         â”‚
â”‚ Rastreab.       â”‚ Sim (fonte clara)â”‚ NÃ£o           â”‚ NÃ£o     â”‚
â”‚                 â”‚                  â”‚               â”‚         â”‚
â”‚ Privacidade     â”‚ Sim (dados local)â”‚ Depende       â”‚ Depende â”‚
â”‚                 â”‚                  â”‚               â”‚         â”‚
â”‚ Casos de Uso    â”‚ Docs, QA,        â”‚ Tarefas       â”‚ Adapt.  â”‚
â”‚                 â”‚ lookup, search   â”‚ especÃ­ficas   â”‚ rÃ¡pida  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Guia de DecisÃ£o: Qual Usar?

```
â”Œâ”€ Dados atualizados frequentemente?
â”‚  â”œâ”€ SIM â†’ RAG (fÃ¡cil update)
â”‚  â””â”€ NÃƒO â†’ Considerar Fine-Tuning
â”‚
â”œâ”€ Precisa citar fontes?
â”‚  â”œâ”€ SIM â†’ RAG (transparÃªncia)
â”‚  â””â”€ NÃƒO â†’ Fine-Tuning/LoRA
â”‚
â”œâ”€ Dados sensÃ­veis/propriet Ã¡rios?
â”‚  â”œâ”€ SIM â†’ RAG (dados locais)
â”‚  â””â”€ NÃƒO â†’ Qualquer um
â”‚
â”œâ”€ Budget limitado?
â”‚  â”œâ”€ SIM â†’ LoRA (<$1000)
â”‚  â””â”€ NÃƒO â†’ Fine-Tuning completo
â”‚
â”œâ”€ LatÃªncia crÃ­tica (<100ms)?
â”‚  â”œâ”€ SIM â†’ Fine-Tuning/LoRA (sem retrieval)
â”‚  â””â”€ NÃƒO â†’ RAG OK (~500ms)
â”‚
â””â”€ Tarefas variadas/multi-domÃ­nio?
   â”œâ”€ SIM â†’ RAG (adaptÃ¡vel)
   â””â”€ NÃƒO â†’ Fine-Tuning especializado
```

#### ImplementaÃ§Ã£o PrÃ¡tica: Combinando Abordagens

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# HYBRID: RAG + Fine-Tuned Generator
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class HybridRAG:
    """
    Combina RAG com generator fine-tuned.
    
    Usa RAG para retrieval + modelo specializado para generation.
    Melhor de dois mundos: conhecimento dinÃ¢mico + precisÃ£o.
    """
    
    def __init__(self, 
                 vector_store,
                 finetuned_llm,  # LLM specializado
                 domain: str = "legal"):
        self.retriever = vector_store.as_retriever(search_kwargs={"k": 5})
        self.llm = finetuned_llm
        self.domain = domain
    
    def query(self, question: str) -> dict:
        """Query hybrid."""
        
        # 1. Retrieve com RAG
        context_docs = self.retriever.get_relevant_documents(question)
        context = "\n".join([doc.page_content for doc in context_docs])
        
        # 2. Generate com modelo fine-tuned
        # O LLM jÃ¡ foi treinado para entender o domÃ­nio
        prompt = self._build_domain_prompt(question, context)
        
        answer = self.llm.generate(prompt)
        
        return {
            "answer": answer,
            "sources": [doc.metadata for doc in context_docs],
            "model_type": "hybrid_rag"
        }
    
    def _build_domain_prompt(self, question: str, context: str) -> str:
        """Prompt especÃ­fico do domÃ­nio."""
        
        if self.domain == "legal":
            return f"""ANÃLISE LEGAL
Contexto de contratos:
{context}

Pergunta: {question}

Analise conforme direito contratual. Cite clÃ¡usulas relevantes."""
        
        elif self.domain == "medical":
            return f"""CONSULT A MÃ‰DICA
Contexto de literatura:
{context}

Paciente: {question}

Baseado em evidÃªncias e guidelines."""
        
        else:
            return f"Contexto:\n{context}\n\nPergunta: {question}"
```

---

## **MÃ“DULO 8: TÃ‰CNICAS AVANÃ‡ADAS**

### Objetivos de Aprendizado
- Implementar RAG multimodal (texto + imagem)
- Entender RAG hÃ­brido (multiple retrieval backends)
- Explorar retriever baseados em modelos avanÃ§ados (ColBERT, Contriever)
- Aplicar compressÃ£o de contexto e tÃ©cnicas de seleÃ§Ã£o

### 8.1 RAG Multimodal

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MULTIMODAL RAG: Texto + Imagens
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from langchain.document_loaders import PyPDFLoader, PDFMinerLoader
from langchain_community.document_loaders.pdf import PyPDFium2Loader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from PIL import Image
import base64
from io import BytesIO

class MultimodalDocumentProcessor:
    """Processa documentos com texto e imagens."""
    
    def __init__(self):
        self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=512)
    
    def extract_text_and_images_from_pdf(self, pdf_path: str) -> list:
        """Extrair texto e imagens de PDF."""
        
        documents = []
        images = []
        
        # Usar PyPDFium2 que mantÃ©m imagens
        loader = PyPDFium2Loader(pdf_path)
        text_docs = loader.load()
        
        # Extrair imagens de PDFs
        import fitz  # PyMuPDF
        pdf_document = fitz.open(pdf_path)
        
        for page_num in range(pdf_document.page_count):
            page = pdf_document[page_num]
            
            # Texto
            text_content = page.get_text()
            if text_content.strip():
                documents.append(Document(
                    page_content=text_content,
                    metadata={
                        "source": pdf_path,
                        "page": page_num,
                        "type": "text"
                    }
                ))
            
            # Imagens
            for img_index, img in enumerate(page.get_images()):
                xref = img[0]
                pix = fitz.Pixmap(pdf_document, xref)
                
                # Converter para base64
                img_data = pix.tobytes("png")
                img_b64 = base64.b64encode(img_data).decode()
                
                documents.append(Document(
                    page_content=f"[IMAGE: {img_index}]",
                    metadata={
                        "source": pdf_path,
                        "page": page_num,
                        "type": "image",
                        "image_base64": img_b64
                    }
                ))
        
        return documents

class MultimodalRetriever:
    """Retriever que processa texto e imagem."""
    
    def __init__(self, vector_store, vision_llm):
        self.vector_store = vector_store
        self.vision_llm = vision_llm  # Claude-3 ou GPT-4V
    
    def retrieve_and_describe_images(self, query: str, k: int = 5):
        """Retrieve documentos e descrever imagens relevantes."""
        
        # Retrieve padrÃ£o (texto)
        text_docs = self.vector_store.similarity_search(query, k=k)
        
        # Se houver imagens, descrever com vision model
        results = []
        
        for doc in text_docs:
            if doc.metadata.get("type") == "image":
                # Descrever imagem
                image_b64 = doc.metadata.get("image_base64")
                
                description = self.vision_llm.generate(
                    f"Descreva esta imagem em relaÃ§Ã£o a: {query}",
                    image=image_b64
                )
                
                results.append({
                    "content": f"[Imagem] {description}",
                    "source": doc.metadata["source"],
                    "type": "image_with_description"
                })
            else:
                results.append({
                    "content": doc.page_content,
                    "source": doc.metadata["source"],
                    "type": "text"
                })
        
        return results

class MultimodalRAGChain:
    """Chain RAG multimodal completo."""
    
    def __init__(self, vector_store, text_llm, vision_llm):
        self.retriever = MultimodalRetriever(vector_store, vision_llm)
        self.text_llm = text_llm
    
    def query(self, question: str) -> str:
        """Query multimodal."""
        
        # Retrieve
        retrieved = self.retriever.retrieve_and_describe_images(question)
        
        # Construir contexto
        context_parts = []
        for item in retrieved:
            if item["type"] == "image_with_description":
                context_parts.append(f"[IMAGEM RELEVANTE]\n{item['content']}")
            else:
                context_parts.append(f"[TEXTO]\n{item['content']}")
        
        context = "\n\n".join(context_parts)
        
        # Generate
        prompt = f"""
        Pergunta: {question}
        
        Contexto (texto e imagens):
        {context}
        
        Responda usando texto e imagens quando relevante.
        """
        
        answer = self.text_llm.generate(prompt)
        
        return answer
```

### 8.2 RAG HÃ­brido (Multiple Backends)

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# HYBRID RAG: Dense + Sparse + Knowledge Graph
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from rank_bm25 import BM25Okapi
import numpy as np
from typing import List, Tuple

class HybridRetriever:
    """Combina Dense (embeddings) + Sparse (BM25) + Graph retrieval."""
    
    def __init__(self, 
                 vector_store,
                 documents: List[str],
                 knowledge_graph = None,
                 weights: dict = None):
        
        self.vector_store = vector_store
        self.documents = documents
        self.kg = knowledge_graph
        
        # Pesos: quanto cada backend contribui
        self.weights = weights or {
            "dense": 0.5,
            "sparse": 0.3,
            "graph": 0.2
        }
        
        # Inicializar BM25
        tokenized_docs = [doc.split() for doc in documents]
        self.bm25 = BM25Okapi(tokenized_docs)
    
    def retrieve_dense(self, query: str, k: int = 5) -> List[Tuple]:
        """Dense retrieval (embeddings)."""
        
        docs = self.vector_store.similarity_search_with_score(query, k=k)
        
        # Normalizar scores [0, 1]
        scores = np.array([score for _, score in docs])
        normalized_scores = (scores - scores.min()) / (scores.max() - scores.min() + 1e-8)
        
        return [(doc.page_content, float(score)) for doc, score in zip(docs, normalized_scores)]
    
    def retrieve_sparse(self, query: str, k: int = 5) -> List[Tuple]:
        """Sparse retrieval (BM25 - keyword)."""
        
        tokenized_query = query.split()
        scores = self.bm25.get_scores(tokenized_query)
        
        # Top-K
        top_indices = np.argsort(scores)[-k:][::-1]
        
        # Normalizar scores
        normalized_scores = scores[top_indices] / (scores[top_indices].max() + 1e-8)
        
        return [
            (self.documents[i], float(normalized_scores[j]))
            for j, i in enumerate(top_indices)
        ]
    
    def retrieve_from_graph(self, query: str, k: int = 5) -> List[Tuple]:
        """Knowledge Graph retrieval (se disponÃ­vel)."""
        
        if not self.kg:
            return []
        
        # Buscar entidades mencionadas na query
        entities = self.kg.extract_entities(query)
        
        # Buscar documentos relacionados a essas entidades
        related_docs = []
        for entity in entities:
            related = self.kg.find_documents_by_entity(entity, k=k)
            related_docs.extend(related)
        
        # Remover duplicatas, ranking por freq
        from collections import Counter
        doc_freq = Counter([doc for doc, _ in related_docs])
        
        # Normalizar
        max_freq = max(doc_freq.values()) if doc_freq else 1
        normalized = [
            (doc, float(count / max_freq))
            for doc, count in doc_freq.most_common(k)
        ]
        
        return normalized
    
    def retrieve_hybrid(self, query: str, k: int = 5) -> List[Tuple]:
        """Combinar todos os retrievers."""
        
        # Retrieve com cada backend
        dense_results = self.retrieve_dense(query, k=k)
        sparse_results = self.retrieve_sparse(query, k=k)
        graph_results = self.retrieve_from_graph(query, k=k)
        
        # Agregar scores ponderados
        combined_scores = {}
        
        for doc, score in dense_results:
            combined_scores[doc] = combined_scores.get(doc, 0) + \
                self.weights["dense"] * score
        
        for doc, score in sparse_results:
            combined_scores[doc] = combined_scores.get(doc, 0) + \
                self.weights["sparse"] * score
        
        for doc, score in graph_results:
            combined_scores[doc] = combined_scores.get(doc, 0) + \
                self.weights["graph"] * score
        
        # Sort e retornar top-K
        sorted_results = sorted(
            combined_scores.items(),
            key=lambda x: x[1],
            reverse=True
        )[:k]
        
        return sorted_results
```

### 8.3 Retrievers AvanÃ§ados: ColBERT e Contriever

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ADVANCED RETRIEVERS: ColBERT, Contriever
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class ColBERTRetriever:
    """
    ColBERT: Late Interaction Retrieval
    
    Vantagem: Compara embeddings TOKEN-LEVEL entre query e doc
    Resultado: Mais preciso que dense retrieval padrÃ£o
    Desvantagem: Mais custoso computacionalmente
    """
    
    def __init__(self, checkpoint: str = "colbert-ir/colbertv2.0"):
        from colbert.infra import ColBERTConfig
        from colbert.modeling.colbert import ColBERT as ColBERTModel
        
        config = ColBERTConfig(
            do_answer_search=False,
            root="/tmp/colbert"
        )
        
        self.model = ColBERTModel(checkpoint=checkpoint, config=config)
    
    def encode_documents(self, documents: List[str]):
        """PrÃ©-computar embeddings de documentos."""
        
        self.doc_embeddings = []
        for doc in documents:
            # ColBERT retorna matriz [num_tokens, hidden_dim]
            embeddings = self.model.encode(doc)
            self.doc_embeddings.append(embeddings)
    
    def retrieve(self, query: str, k: int = 5):
        """Retrieve usando ColBERT."""
        
        # Encode query (tambÃ©m matriz de tokens)
        query_embeddings = self.model.encode(query)  # [Q, hidden_dim]
        
        # Compute similarity scores (late interaction)
        scores = []
        
        for doc_emb in self.doc_embeddings:
            # Query: [Q_tokens, hidden]
            # Doc: [D_tokens, hidden]
            # Score = max over doc tokens of (max similarity to query tokens)
            
            similarity_matrix = query_embeddings @ doc_emb.T  # [Q, D]
            max_sim_per_query_token = similarity_matrix.max(dim=1)[0]  # [Q]
            score = max_sim_per_query_token.mean()  # MÃ©dia
            
            scores.append(score)
        
        # Top-K
        top_indices = np.argsort(scores)[-k:][::-1]
        
        return [(self.documents[i], scores[i]) for i in top_indices]


class ContrieverRetriever:
    """
    Contriever: Contrastive Learning Dense Retriever
    
    Treinado com contrastive learning em dados nÃ£o-supervisionados
    Ã“timo para zero-shot retrieval sem fine-tuning
    """
    
    def __init__(self, model_name: str = "facebook/contriever"):
        from sentence_transformers import SentenceTransformer
        
        self.model = SentenceTransformer(model_name)
    
    def retrieve(self, query: str, documents: List[str], k: int = 5):
        """Retrieve com Contriever."""
        
        # Encode query e docs
        query_embedding = self.model.encode(query, convert_to_tensor=True)
        doc_embeddings = self.model.encode(documents, convert_to_tensor=True)
        
        # Similaridade coseno
        from torch.nn.functional import cosine_similarity
        scores = cosine_similarity(query_embedding.unsqueeze(0), 
                                   doc_embeddings)[0]
        
        # Top-K
        top_indices = scores.argsort(descending=True)[:k]
        
        return [
            (documents[i], float(scores[i]))
            for i in top_indices
        ]
```

### 8.4 CompressÃ£o de Contexto

```python
class ContextCompressor:
    """TÃ©cnicas para comprimir contexto sem perder informaÃ§Ã£o."""
    
    def __init__(self, compression_llm):
        self.llm = compression_llm
    
    def compress_by_summarization(self, context: str, 
                                  compression_ratio: float = 0.5) -> str:
        """Resumir contexto para ocupar menos tokens."""
        
        target_length = int(len(context.split()) * compression_ratio)
        
        prompt = f"""
        Resuma o seguinte contexto em ~{target_length} palavras,
        mantendo informaÃ§Ãµes crÃ­ticas:
        
        {context}
        """
        
        summary = self.llm.generate(prompt)
        return summary
    
    def compress_by_extraction(self, context: str, query: str) -> str:
        """Extrair apenas as partes relevantes para a query."""
        
        prompt = f"""
        Contexto:
        {context}
        
        Query: {query}
        
        Extraia APENAS as sentenÃ§as do contexto relevantes para a query.
        Mantenha ordem original. Retorne apenas as sentenÃ§as, nenhum texto extra.
        """
        
        extracted = self.llm.generate(prompt)
        return extracted
    
    def compress_by_quantization(self, context: str) -> str:
        """Quantizar informaÃ§Ã£o (simplificar linguagem)."""
        
        prompt = f"""
        Simplifique o seguinte texto, usando termos mais simples
        e estrutura mais concisa:
        
        {context}
        """
        
        simplified = self.llm.generate(prompt)
        return simplified
```

---

## **RESUMO EXECUTIVO**

### Conceitos Chave

**RAG (Retrieval-Augmented Generation)** Ã© um paradigma que combina:
1. **Retriever**: Busca semÃ¢ntica em base de conhecimento
2. **Generator**: LLM que gera respostas usando contexto recuperado

### BenefÃ­cios Principais
- âœ… Reduz alucinaÃ§Ãµes (respostas grounded em dados reais)
- âœ… Conhecimento sempre atualizado (sem retreinar)
- âœ… Rastreabilidade (citations dos documentos)
- âœ… Custo-efetivo (usa modelos prÃ©-treinados)

### LimitaÃ§Ãµes
- âš ï¸ LatÃªncia de retrieval (~500ms)
- âš ï¸ Qualidade depende do retriever ("garbage in, garbage out")
- âš ï¸ Contexto limitado pela janela do LLM

### Stack TecnolÃ³gico Recomendado

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ APLICAÃ‡ÃƒO (Streamlit, FastAPI, etc.)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ LangChain / LlamaIndex (OrquestraÃ§Ã£o)   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ OpenAI GPT-4 / Claude 3 (LLM)           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Chroma / FAISS (Vector Store)           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Sentence Transformers (Embeddings)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Documentos (PDFs, Wikis, APIs)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### PrÃ³ximos Passos

1. **Implementar MVP**: Comece simples (1 retriever, 1 LLM)
2. **Avaliar performance**: Use RAGAS para mÃ©tricas
3. **Otimizar**: Ajuste chunking, embedding model, prompts
4. **Escalar**: Adicione monitoramento, caching, versionamento
5. **Experimentar**: Teste RAG hÃ­brido, multimodal, avanÃ§ado

---

## **RECURSOS ADICIONAIS**

### Papers Fundamentais

1. **Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks** (Lewis et al., 2020)
   - Paper seminal que introduz RAG
   - https://arxiv.org/abs/2005.11401

2. **A Comprehensive Survey of Retrieval-Augmented Generation** (2024)
   - Overview estado-da-arte
   - https://arxiv.org/abs/2410.12837

3. **RAGAS: Automated Evaluation of Retrieval Augmented Generation** (2023)
   - Framework de avaliaÃ§Ã£o
   - https://arxiv.org/abs/2309.15217

4. **Dense Passage Retrieval for Open-Domain Question Answering** (Karpukhin et al., 2020)
   - DPR: Dense retrieval foundational
   - https://arxiv.org/abs/2004.04906

### Bibliotecas Python

- **LangChain**: OrquestraÃ§Ã£o, chains, memoria
- **LlamaIndex**: IndexaÃ§Ã£o, retrieval especializado
- **Chroma**: Vector store simples e rÃ¡pido
- **FAISS**: Vector search em escala
- **Weaviate**: Vector DB enterprise
- **Sentence-Transformers**: Embeddings
- **RAGAS**: AvaliaÃ§Ã£o automÃ¡tica

### Tutoriais Online

- LangChain Documentation: https://python.langchain.com
- LlamaIndex Docs: https://docs.llamaindex.ai
- DeepLearning.AI Short Courses (RAG)
- YouTube: Josh Maker, Matt Shumer, etc.

### Ferramentas de Prototipagem

- **Hugging Face**: Modelos prÃ©-treinados
- **LiteLLM**: IntegraÃ§Ã£o com mÃºltiplos LLMs
- **Streamlit**: UI rÃ¡pida para demos
- **Gradio**: Interface para modelos

---

## **CONCLUSÃƒO**

RAG Ã© um paradigma poderoso e prÃ¡tico que resolve limitaÃ§Ãµes fundamentais dos LLMs modernos. Ao combinar retrieval dinÃ¢mico com generaÃ§Ã£o em tempo de inferÃªncia, RAG permite sistemas mais precisos, atualizÃ¡veis e interpretÃ¡veis.

O futuro de RAG estÃ¡ em:
- **HÃ­brido**: Combinando mÃºltiplos retrievers (dense + sparse + graph)
- **Multimodal**: Integrando texto, imagem, cÃ³digo, tabelas
- **Agentic**: Usando RAG em agentes que raciocinam e planejam
- **Especializado**: Fine-tuning de componentes para domÃ­nios especÃ­ficos

Comece simples, experimente, avalie, e escale gradualmente conforme ganhar experiÃªncia!

---

**Ãšltima atualizaÃ§Ã£o**: Novembro 2025  
**NÃ­vel**: AvanÃ§ado (prÃ©-requisitos: Python, NLP bÃ¡sico, famili aridade com LLMs)  
**Tempo estimado de leitura/aprendizado**: 20-30 horas
